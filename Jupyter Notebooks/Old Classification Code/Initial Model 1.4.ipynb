{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc6c140",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d174e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF \n",
    "# !pip install transformers torch\n",
    "# !pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64d74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import logging\n",
    "import spacy\n",
    "import os\n",
    "import string\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
    "from scipy.sparse import hstack\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel, TFAutoModel\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b417a19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab59a12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>group</th>\n",
       "      <th>text before lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>Provincies willen aan de slag met versoepeling...</td>\n",
       "      <td>provincie willen slag versoepeling stikstofreg...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>provincies willen slag versoepeling stikstofre...</td>\n",
       "      <td>[(provincie, NOUN), (willen, VERB), (slag, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Het draait allemaal om de drempelwaarde voor e...</td>\n",
       "      <td>draaien allemaal drempelwaran stikstofvergunni...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>draait allemaal drempelwaarde stikstofvergunni...</td>\n",
       "      <td>[(draaien, VERB), (allemaal, ADV), (drempelwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Met een hogere drempelwaarde zouden minder ver...</td>\n",
       "      <td>hoog drempelwaard vergunning [NEWLINE] aangevo...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hogere drempelwaarde vergunningen [NEWLINE] aa...</td>\n",
       "      <td>[(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In het hoofdlijnenakkoord hebben de vier coali...</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartij afsprek...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartijen afges...</td>\n",
       "      <td>[(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De ondergrens is al langer onderwerp van discu...</td>\n",
       "      <td>ondergren lang onderwerp discussie huidig Nede...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>ondergrens langer onderwerp discussie huidige ...</td>\n",
       "      <td>[(ondergren, VERB), (lang, ADJ), (onderwerp, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>2</th>\n",
       "      <td>Telgenkamp vestigt haar hoop voor de korte ter...</td>\n",
       "      <td>telgenkamp vestigen hoop kort termijn twee cru...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>telgenkamp vestigt hoop korte termijn twee cru...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <th>1</th>\n",
       "      <td>Waarom verzekeraars inkomsten uit zwart werk w...</td>\n",
       "      <td>verzekeraar inkomst zwart werk vergoeden [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraars inkomsten zwart werk vergoeden [N...</td>\n",
       "      <td>[(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">120</th>\n",
       "      <th>1</th>\n",
       "      <td>Verzekeraar wil klant helpen met zorgbemiddeli...</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verzekeraar wil wachtende patiënt aan snelle z...</td>\n",
       "      <td>verzekeraar wachten patiënt snel zorg helpen [...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar wachtende patiënt snelle zorg help...</td>\n",
       "      <td>[(verzekeraar, NOUN), (wachten, VERB), (patiën...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zorgbemiddeling is geen wondermiddel, maar kan...</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zeggen Haa...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zegt haarl...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             original_text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             Provincies willen aan de slag met versoepeling...   \n",
       "           2             Het draait allemaal om de drempelwaarde voor e...   \n",
       "           3             Met een hogere drempelwaarde zouden minder ver...   \n",
       "           4             In het hoofdlijnenakkoord hebben de vier coali...   \n",
       "           5             De ondergrens is al langer onderwerp van discu...   \n",
       "...                                                                    ...   \n",
       "118        2             Telgenkamp vestigt haar hoop voor de korte ter...   \n",
       "119        1             Waarom verzekeraars inkomsten uit zwart werk w...   \n",
       "120        1             Verzekeraar wil klant helpen met zorgbemiddeli...   \n",
       "           2             Verzekeraar wil wachtende patiënt aan snelle z...   \n",
       "           3             Zorgbemiddeling is geen wondermiddel, maar kan...   \n",
       "\n",
       "                                                                      text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincie willen slag versoepeling stikstofreg...   \n",
       "           2             draaien allemaal drempelwaran stikstofvergunni...   \n",
       "           3             hoog drempelwaard vergunning [NEWLINE] aangevo...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartij afsprek...   \n",
       "           5             ondergren lang onderwerp discussie huidig Nede...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigen hoop kort termijn twee cru...   \n",
       "119        1             verzekeraar inkomst zwart werk vergoeden [NEWL...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachten patiënt snel zorg helpen [...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zeggen Haa...   \n",
       "\n",
       "                                   group  \\\n",
       "article_id paragraph_id                    \n",
       "1          1             Bouw & Vastgoed   \n",
       "           2             Bouw & Vastgoed   \n",
       "           3             Bouw & Vastgoed   \n",
       "           4             Bouw & Vastgoed   \n",
       "           5             Bouw & Vastgoed   \n",
       "...                                  ...   \n",
       "118        2                        Zorg   \n",
       "119        1                        Zorg   \n",
       "120        1                        Zorg   \n",
       "           2                        Zorg   \n",
       "           3                        Zorg   \n",
       "\n",
       "                                                 text before lemmatization  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincies willen slag versoepeling stikstofre...   \n",
       "           2             draait allemaal drempelwaarde stikstofvergunni...   \n",
       "           3             hogere drempelwaarde vergunningen [NEWLINE] aa...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartijen afges...   \n",
       "           5             ondergrens langer onderwerp discussie huidige ...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigt hoop korte termijn twee cru...   \n",
       "119        1             verzekeraars inkomsten zwart werk vergoeden [N...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachtende patiënt snelle zorg help...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zegt haarl...   \n",
       "\n",
       "                                                                  pos_tags  \n",
       "article_id paragraph_id                                                     \n",
       "1          1             [(provincie, NOUN), (willen, VERB), (slag, NOU...  \n",
       "           2             [(draaien, VERB), (allemaal, ADV), (drempelwar...  \n",
       "           3             [(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...  \n",
       "           4             [(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...  \n",
       "           5             [(ondergren, VERB), (lang, ADJ), (onderwerp, N...  \n",
       "...                                                                    ...  \n",
       "118        2             [(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...  \n",
       "119        1             [(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...  \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...  \n",
       "           2             [(verzekeraar, NOUN), (wachten, VERB), (patiën...  \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...  \n",
       "\n",
       "[374 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pre-processed data.csv').set_index(['article_id' , 'paragraph_id'], inplace=False)\n",
    "df['pos_tags'] = df['pos_tags'].apply(ast.literal_eval)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360db14",
   "metadata": {},
   "source": [
    "# POS-tags one-hot encoding & ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444f636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_to_features(pos_tags):\n",
    "    \"\"\"Convert list of (word, POS) tuples into a dictionary of POS tag counts.\"\"\"\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "    return dict(pos_counts)\n",
    "\n",
    "def pos_to_ngrams(pos_tags, n=2):\n",
    "    \"\"\"Convert a list of POS-tag tuples into n-gram strings.\"\"\"\n",
    "    pos_sequence = [tag for _, tag in pos_tags]  # Extract only POS tags\n",
    "    ngrams = ['_'.join(pos_sequence[i:i+n]) for i in range(len(pos_sequence)-n+1)]\n",
    "    return ' '.join(ngrams)  # Convert to space-separated string for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d58093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (261, 6376), Validation shape: (56, 6376), Test shape: (57, 6376)\n"
     ]
    }
   ],
   "source": [
    "X = df[['text', 'pos_tags']].copy() # Testing on df\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"group\"])\n",
    "\n",
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split the data into text and POS-tags\n",
    "X_train_text, X_train_pos = X_train['text'], X_train['pos_tags']\n",
    "X_val_text, X_val_pos = X_val['text'], X_val['pos_tags']\n",
    "X_test_text, X_test_pos = X_test['text'], X_test['pos_tags']\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "X_val_text = vectorizer.transform(X_val_text)\n",
    "X_test_text = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Convert POS tags to feature dictionaries\n",
    "X_train_pos_features = X_train_pos.apply(pos_to_features)\n",
    "X_val_pos_features = X_val_pos.apply(pos_to_features)\n",
    "X_test_pos_features = X_test_pos.apply(pos_to_features)\n",
    "\n",
    "# Vectorize POS features (Fit on train, Transform on val/test)\n",
    "pos_vectorizer = DictVectorizer(sparse=True)\n",
    "X_train_pos = pos_vectorizer.fit_transform(X_train_pos_features)\n",
    "X_val_pos = pos_vectorizer.transform(X_val_pos_features)\n",
    "X_test_pos = pos_vectorizer.transform(X_test_pos_features)\n",
    "\n",
    "# Scale POS features (Fit on train, Transform on val/test)\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_pos = scaler.fit_transform(X_train_pos)\n",
    "X_val_pos = scaler.transform(X_val_pos)\n",
    "X_test_pos = scaler.transform(X_test_pos)\n",
    "\n",
    "# Combine TF-IDF and POS tag features\n",
    "X_train_combined = hstack([X_train_text, X_train_pos])\n",
    "X_val_combined = hstack([X_val_text, X_val_pos])\n",
    "X_test_combined = hstack([X_test_text, X_test_pos])\n",
    "\n",
    "# Final shape check\n",
    "print(f\"Train shape: {X_train_combined.shape}, Validation shape: {X_val_combined.shape}, Test shape: {X_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ae364-ee83-4271-85db-08e4cc710806",
   "metadata": {},
   "source": [
    "**Now let's test them on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36247b95-7915-4030-9aff-65b969f8269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 5, Validation Score: 0.5714285714285714\n",
      "Depth: 10, Validation Score: 0.6607142857142857\n",
      "Depth: 15, Validation Score: 0.6607142857142857\n",
      "Depth: 20, Validation Score: 0.7142857142857143\n",
      "Depth: 25, Validation Score: 0.6964285714285714\n",
      "Depth: None, Validation Score: 0.6607142857142857\n",
      "\n",
      "Best Depth: 20, Best Validation Score: 0.7142857142857143\n",
      "\n",
      "Test Accuracy: 0.5614035087719298\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75        21\n",
      "           1       0.45      0.59      0.51        17\n",
      "           2       0.36      0.40      0.38        10\n",
      "           3       0.60      0.33      0.43         9\n",
      "\n",
      "    accuracy                           0.56        57\n",
      "   macro avg       0.55      0.51      0.52        57\n",
      "weighted avg       0.58      0.56      0.56        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(X_train_combined, y_train)\n",
    "    val_score = classifier.score(X_val_combined, y_val)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_combined)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556bb76a-bde5-49fe-b24a-8b589024a160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Score: 0.4642857142857143\n",
      "Kernel: linear, C: 1, Validation Score: 0.7321428571428571\n",
      "Kernel: linear, C: 10, Validation Score: 0.75\n",
      "Kernel: rbf, C: 0.1, Validation Score: 0.42857142857142855\n",
      "Kernel: rbf, C: 1, Validation Score: 0.5178571428571429\n",
      "Kernel: rbf, C: 10, Validation Score: 0.6964285714285714\n",
      "Kernel: poly, C: 0.1, Validation Score: 0.48214285714285715\n",
      "Kernel: poly, C: 1, Validation Score: 0.5714285714285714\n",
      "Kernel: poly, C: 10, Validation Score: 0.5892857142857143\n",
      "\n",
      "Best Kernel: linear, Best C: 10, Best Validation Score: 0.75\n",
      "\n",
      "Test Accuracy: 0.6666666666666666\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72        21\n",
      "           1       0.55      0.71      0.62        17\n",
      "           2       0.70      0.70      0.70        10\n",
      "           3       0.71      0.56      0.63         9\n",
      "\n",
      "    accuracy                           0.67        57\n",
      "   macro avg       0.68      0.66      0.66        57\n",
      "weighted avg       0.68      0.67      0.67        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_combined, y_train)\n",
    "        val_score = classifier.score(X_val_combined, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_combined)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deae51b0-1312-43fd-b60d-2a733e7541b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Score: 0.375\n",
      "Kernel: linear, C: 1, Validation Score: 0.8035714285714286\n",
      "Kernel: linear, C: 10, Validation Score: 0.7678571428571429\n",
      "Kernel: rbf, C: 0.1, Validation Score: 0.375\n",
      "Kernel: rbf, C: 1, Validation Score: 0.6785714285714286\n",
      "Kernel: rbf, C: 10, Validation Score: 0.7142857142857143\n",
      "Kernel: poly, C: 0.1, Validation Score: 0.375\n",
      "Kernel: poly, C: 1, Validation Score: 0.6071428571428571\n",
      "Kernel: poly, C: 10, Validation Score: 0.6428571428571429\n",
      "\n",
      "Best Kernel: linear, Best C: 1, Best Validation Score: 0.8035714285714286\n",
      "\n",
      "Test Accuracy: 0.7719298245614035\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        21\n",
      "           1       0.74      0.82      0.78        17\n",
      "           2       0.62      0.80      0.70        10\n",
      "           3       0.86      0.67      0.75         9\n",
      "\n",
      "    accuracy                           0.77        57\n",
      "   macro avg       0.77      0.76      0.76        57\n",
      "weighted avg       0.79      0.77      0.77        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing only text\n",
    "\n",
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_text, y_train)\n",
    "        val_score = classifier.score(X_val_text, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_text, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_text)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c891c-6b89-4370-8f72-2abceb9e89fe",
   "metadata": {},
   "source": [
    "**Now we test for pos_tag ngrams rather than one-hot encodings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf55f15-3e11-4404-a4c9-13935f30761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (261, 6514), Validation shape: (56, 6514), Test shape: (57, 6514)\n"
     ]
    }
   ],
   "source": [
    "# Testing for ngrams\n",
    "\n",
    "X = df[['text', 'pos_tags']].copy() # Testing on df\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"group\"])\n",
    "\n",
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split the data into text and POS-tags\n",
    "X_train_text, X_train_pos = X_train['text'], X_train['pos_tags']\n",
    "X_val_text, X_val_pos = X_val['text'], X_val['pos_tags']\n",
    "X_test_text, X_test_pos = X_test['text'], X_test['pos_tags']\n",
    "\n",
    "\n",
    "# Split the data into text and POS-tags and construct ngrams\n",
    "X_train_text, X_train_pos = X_train['text'], X_train['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "X_val_text, X_val_pos = X_val['text'], X_val['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "X_test_text, X_test_pos = X_test['text'], X_test['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "\n",
    "\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "X_val_text = vectorizer.transform(X_val_text)\n",
    "X_test_text = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Convert POS n-grams to TF-IDF\n",
    "pos_vectorizer = TfidfVectorizer()\n",
    "X_train_pos = pos_vectorizer.fit_transform(X_train_pos)\n",
    "X_val_pos = pos_vectorizer.transform(X_val_pos)\n",
    "X_test_pos = pos_vectorizer.transform(X_test_pos)\n",
    "\n",
    "# Scale POS features (Fit on train, Transform on val/test)\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_pos = scaler.fit_transform(X_train_pos)\n",
    "X_val_pos = scaler.transform(X_val_pos)\n",
    "X_test_pos = scaler.transform(X_test_pos)\n",
    "\n",
    "# Combine TF-IDF and POS tag features\n",
    "X_train_combined = hstack([X_train_text, X_train_pos])\n",
    "X_val_combined = hstack([X_val_text, X_val_pos])\n",
    "X_test_combined = hstack([X_test_text, X_test_pos])\n",
    "\n",
    "# Final shape check\n",
    "print(f\"Train shape: {X_train_combined.shape}, Validation shape: {X_val_combined.shape}, Test shape: {X_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281c2a37-6630-408a-ad46-c7e81a619e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 5, Validation Score: 0.5535714285714286\n",
      "Depth: 10, Validation Score: 0.6071428571428571\n",
      "Depth: 15, Validation Score: 0.6071428571428571\n",
      "Depth: 20, Validation Score: 0.6071428571428571\n",
      "Depth: 25, Validation Score: 0.625\n",
      "Depth: None, Validation Score: 0.6428571428571429\n",
      "\n",
      "Best Depth: None, Best Validation Score: 0.6428571428571429\n",
      "\n",
      "Test Accuracy: 0.5964912280701754\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74        21\n",
      "           1       0.52      0.65      0.58        17\n",
      "           2       0.45      0.50      0.48        10\n",
      "           3       0.67      0.22      0.33         9\n",
      "\n",
      "    accuracy                           0.60        57\n",
      "   macro avg       0.59      0.53      0.53        57\n",
      "weighted avg       0.61      0.60      0.58        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(X_train_combined, y_train)\n",
    "    val_score = classifier.score(X_val_combined, y_val)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_combined)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21cca98b-a772-4af0-af6c-9639197ae09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Score: 0.5178571428571429\n",
      "Kernel: linear, C: 1, Validation Score: 0.5535714285714286\n",
      "Kernel: linear, C: 10, Validation Score: 0.5714285714285714\n",
      "Kernel: rbf, C: 0.1, Validation Score: 0.375\n",
      "Kernel: rbf, C: 1, Validation Score: 0.5178571428571429\n",
      "Kernel: rbf, C: 10, Validation Score: 0.5178571428571429\n",
      "Kernel: poly, C: 0.1, Validation Score: 0.375\n",
      "Kernel: poly, C: 1, Validation Score: 0.5178571428571429\n",
      "Kernel: poly, C: 10, Validation Score: 0.5714285714285714\n",
      "\n",
      "Best Kernel: linear, Best C: 10, Best Validation Score: 0.5714285714285714\n",
      "\n",
      "Test Accuracy: 0.47368421052631576\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.62      0.60        21\n",
      "           1       0.47      0.47      0.47        17\n",
      "           2       0.36      0.50      0.42        10\n",
      "           3       0.25      0.11      0.15         9\n",
      "\n",
      "    accuracy                           0.47        57\n",
      "   macro avg       0.42      0.43      0.41        57\n",
      "weighted avg       0.46      0.47      0.46        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_combined, y_train)\n",
    "        val_score = classifier.score(X_val_combined, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_combined, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_combined)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66be018-2a5a-4b41-9ac8-209782683977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Score: 0.375\n",
      "Kernel: linear, C: 1, Validation Score: 0.8035714285714286\n",
      "Kernel: linear, C: 10, Validation Score: 0.7678571428571429\n",
      "Kernel: rbf, C: 0.1, Validation Score: 0.375\n",
      "Kernel: rbf, C: 1, Validation Score: 0.6785714285714286\n",
      "Kernel: rbf, C: 10, Validation Score: 0.7142857142857143\n",
      "Kernel: poly, C: 0.1, Validation Score: 0.375\n",
      "Kernel: poly, C: 1, Validation Score: 0.6071428571428571\n",
      "Kernel: poly, C: 10, Validation Score: 0.6428571428571429\n",
      "\n",
      "Best Kernel: linear, Best C: 1, Best Validation Score: 0.8035714285714286\n",
      "\n",
      "Test Accuracy: 0.7719298245614035\n",
      "\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        21\n",
      "           1       0.74      0.82      0.78        17\n",
      "           2       0.62      0.80      0.70        10\n",
      "           3       0.86      0.67      0.75         9\n",
      "\n",
      "    accuracy                           0.77        57\n",
      "   macro avg       0.77      0.76      0.76        57\n",
      "weighted avg       0.79      0.77      0.77        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing only text\n",
    "\n",
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_text, y_train)\n",
    "        val_score = classifier.score(X_val_text, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_text, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_text)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bd02a-8eec-4b9d-84d6-fdd772cb115f",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749b76f-1d11-4f0f-aa36-c4a96ef625dd",
   "metadata": {},
   "source": [
    "Since the process of tuning a classifier tends to not change much, we create a function for every type of classifier so that we can tune them without needing to re-write the code every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc89e3a-4ac5-44cd-9027-305fabf0b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_SVM(df: pd.DataFrame,\n",
    "             testing_ratio: float = 0.15, \n",
    "             vectorization_within_folds: bool = False,\n",
    "             k_values: list = [2, 3, 5, 10, 20], \n",
    "             embedding: str = \"tf-idf\",\n",
    "             pos: str = \"none\", \n",
    "             n: list = [2,3,4,5]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    best_score = 0\n",
    "    \n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]    \n",
    "\n",
    "    if pos in [\"one-hot\", \"ngram\"]:\n",
    "        X = df[['text', 'pos_tags']].copy()\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(df[\"group\"])\n",
    "\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        if pos == \"one-hot\":\n",
    "            # Split the data into text and POS-tags\n",
    "            X_train_text, X_train_pos = X_train['text'], X_train['pos_tags']\n",
    "            X_val_text, X_val_pos = X_val['text'], X_val['pos_tags']\n",
    "            X_test_text, X_test_pos = X_test['text'], X_test['pos_tags']\n",
    "\n",
    "            # Convert POS tags to feature dictionaries\n",
    "            X_train_pos_features = X_train_pos.apply(pos_to_features)\n",
    "            X_val_pos_features = X_val_pos.apply(pos_to_features)\n",
    "            X_test_pos_features = X_test_pos.apply(pos_to_features)\n",
    "        else:\n",
    "            # Split the data into text and POS-tags and construct ngrams\n",
    "            X_train_text, X_train_pos = X_train['text'], X_train['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "            X_val_text, X_val_pos = X_val['text'], X_val['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "            X_test_text, X_test_pos = X_test['text'], X_test['pos_tags'].apply(lambda x: pos_to_ngrams(x, n=2))\n",
    "\n",
    "            # Convert POS n-grams to TF-IDF\n",
    "            pos_vectorizer = TfidfVectorizer()\n",
    "            X_train_pos = pos_vectorizer.fit_transform(X_train_pos)\n",
    "            X_val_pos = pos_vectorizer.transform(X_val_pos)\n",
    "            X_test_pos = pos_vectorizer.transform(X_test_pos)\n",
    "\n",
    "        # Convert text to TF-IDF representation\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "        X_val_text = vectorizer.transform(X_val_text)\n",
    "        X_test_text = vectorizer.transform(X_test_text)\n",
    "        \n",
    "        # Vectorize POS features (Fit on train, Transform on val/test)\n",
    "        pos_vectorizer = DictVectorizer(sparse=True)\n",
    "        X_train_pos = pos_vectorizer.fit_transform(X_train_pos_features)\n",
    "        X_val_pos = pos_vectorizer.transform(X_val_pos_features)\n",
    "        X_test_pos = pos_vectorizer.transform(X_test_pos_features)\n",
    "        \n",
    "        # Scale POS features (Fit on train, Transform on val/test)\n",
    "        scaler = MaxAbsScaler()\n",
    "        X_train_pos = scaler.fit_transform(X_train_pos)\n",
    "        X_val_pos = scaler.transform(X_val_pos)\n",
    "        X_test_pos = scaler.transform(X_test_pos)\n",
    "        \n",
    "        # Combine TF-IDF and POS tag features\n",
    "        X_train = hstack([X_train_text, X_train_pos])\n",
    "        X_val = hstack([X_val_text, X_val_pos])\n",
    "        X_test = hstack([X_test_text, X_test_pos])\n",
    "\n",
    "\n",
    "    else: # POS-tags are not used in the classification\n",
    "        X = df['text']  # Feature: text column\n",
    "        y = df['group']  # Label: group column\n",
    "    \n",
    "        # Split the data into training (85%) and test (15%) sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testing_ratio, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for C in C_values:\n",
    "                for kernel in kernels:\n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, C: {C}, Kernel: {kernel}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_C = C\n",
    "                        best_kernel = kernel\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for C in C_values:\n",
    "            for kernel in kernels:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, C: {C}, Kernel: {kernel}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_C = C\n",
    "                    best_kernel = kernel\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best C: {best_C}, Best kernel: {best_kernel}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d794ae-b245-4602-977d-7e046eb426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_naive_bayes(df: pd.DataFrame,\n",
    "                     testing_ratio: float = 0.15, \n",
    "                     vectorization_within_folds: bool = False,\n",
    "                     k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    best_score = 0\n",
    "    \n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]  \n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                for fit_prior_value in fit_prior_values:\n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_alpha = alpha\n",
    "                        best_fit_prior = fit_prior_value\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            for fit_prior_value in fit_prior_values:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_alpha = alpha\n",
    "                    best_fit_prior = fit_prior_value\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21eee1-076b-4822-bfab-41694e2a49a9",
   "metadata": {},
   "source": [
    "# Overarching Model Tuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af11ce07-7f00-4ef9-a361-fa923082a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_random_forest(df: pd.DataFrame,\n",
    "                       testing_ratio: float = 0.15, \n",
    "                       vectorization_within_folds: bool = False,\n",
    "                       k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the Random Forest using stratified K-fold cross validation\n",
    "    best_depth = None\n",
    "    best_score = 0\n",
    "    depth_values = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "    # depth_values = [5, 10]  # For faster tests\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for depth in depth_values:\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_tfidf, y_train)\n",
    "                y_pred = classifier.predict(X_val_tfidf)\n",
    "                mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                print(f\"K: {k}, Depth: {depth}, Validation Accuracy: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_depth = depth\n",
    "                    best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for depth in depth_values:\n",
    "            scores = []\n",
    "\n",
    "            if vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train, y_train):\n",
    "                    # Split the raw text data for the current fold\n",
    "                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "\n",
    "            if not vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold)\n",
    "                    \n",
    "            scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Depth: {depth}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_depth = depth\n",
    "                best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best depth: {best_depth}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d0747-b529-4e9e-8df3-5db70ab1d4a4",
   "metadata": {},
   "source": [
    "Expected parameter values: <br>\n",
    "> - _classifier_: Selects the type of classifier from amongst the following: [\"SVM\", \"NB\", \"RF\"].\n",
    "> - _embedding_:\n",
    "> - _pos_:\n",
    "> - _n_:\n",
    "> > Note that if _pos_$\\neq$\"ngram\", n is not used and its value is irrelevant.\n",
    "> - _testing_ratio_: The ratio of the data that is reserved for testing. Any floating point in the inclusive interval [0, 1].\n",
    "> > Note that if $k=1$, the size of the validation set is assumed to be equal to the size of the testing set, specified by _testing_ratio_.\n",
    "> - _vectorization_within_folds_: Would you like to vectorize each individual fold rather than vectorizing the entire training set once? [True, False].\n",
    "> - _show_class_accuracy_: Would you like the accuracy per class to be displayed? [True, False].\n",
    "> - _show_confusion_matrix_: Would you like the resulting confusion matrix to be displayed? [True, False].\n",
    "> - _k_values_: All values of k which are tested for stratified k-fold cross validation. Any list containing only positive integers.\n",
    "> - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22ddf9f-5998-40db-a1c4-807d2c223a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df: pd.DataFrame,\n",
    "                model_type: str = \"SVM\", \n",
    "                embedding: str = \"tf-idf\",\n",
    "                pos: str = \"none\",\n",
    "                n: list = [2,3,4,5],\n",
    "                testing_ratio: float = 0.15, \n",
    "                vectorization_within_folds: bool = False, \n",
    "                show_class_accuracy: bool = True, \n",
    "                show_confusion_matrix: bool = True,\n",
    "                k_values: list = [2, 3, 5, 10, 20],):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    print(f\"Tuning {model_type} classifier with a train/test split of {1-testing_ratio}/{testing_ratio} \\n\")\n",
    "    \n",
    "    #Raise appropriate error message in case of a faulty parameter value\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(f\"Invalid input data. Please ensure df is a Pandas DataFrame\")\n",
    "    if model_type not in [\"SVM\", \"NB\", \"RF\"]:\n",
    "        raise ValueError(f\"Invalid model_type. Choose from {'SVM', 'NB', 'RF'}\")\n",
    "    if testing_ratio < 0 or testing_ratio > 1:\n",
    "        raise ValueError(f\"Invalid testing ratio. Choose a value in the inclusive interval [0,1]\")\n",
    "    if type(vectorization_within_folds) != bool:\n",
    "        raise ValueError(f\"Invalid vectorization_within_folds value. Please ensure vectorization_within_folds is boolean\")\n",
    "    if type(show_class_accuracy) != bool:\n",
    "        raise ValueError(f\"Invalid show_class_accuracy value. Please ensure show_class_accuracy is boolean\")\n",
    "    if type(show_confusion_matrix) != bool:\n",
    "        raise ValueError(f\"Invalid show_confusion_matrix value. Please ensure show_confusion_matrix is boolean\")\n",
    "    if not all(isinstance(x, int) and x > 0 for x in k_values):\n",
    "        raise ValueError(f\"Invalid k_values. Please ensure all entries in k_values are positive integers\")\n",
    "    if 1 in k_values and vectorization_within_folds:\n",
    "        raise ValueError(f\"If k_values contains 1, vectorization_within_folds must be False since k=1 implies standard hold-out cross-validation, for which vectorization_within_folds must be False\")\n",
    "    if embedding not in [\"tf-idf\", \"BERTje\"]:\n",
    "        raise ValueError(f'Invalid embedding. Choose from [\"tf-idf\", \"BERTje\"]')\n",
    "    if pos not in [\"none\", \"one-hot\", \"ngram\"]:\n",
    "        raise ValueError(f'Invalid pos. Choose from [\"none\", \"one-hot\", \"ngram\"]')\n",
    "    if type(n) != list or any(not isinstance(x, int) or x < 2 for x in n):\n",
    "        raise ValueError(f\"Invalid n value. Please ensure n is a list of integers that are no smaller than 2\")\n",
    "\n",
    "    model_mapping = {\"SVM\": SVC, \"NB\": MultinomialNB , \"RF\": RandomForestClassifier}\n",
    "    ModelClass = model_mapping[model_type]\n",
    "    \n",
    "    #Best classifier performance and number of stratified folds so far\n",
    "    best_score = 0\n",
    "    best_k = None\n",
    "\n",
    "    #RF hyperparameters:\n",
    "    best_depth = None\n",
    "    depth_values = [5, 10, 15, 20, 25, None]\n",
    "\n",
    "    #SVM hyperparameters:\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    kernel_values = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]\n",
    "\n",
    "    #NB hyperparameters:\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]\n",
    "    \n",
    "    if model_type == \"SVM\":\n",
    "        results = tune_SVM(df=df, testing_ratio=testing_ratio, \n",
    "                           vectorization_within_folds=vectorization_within_folds, \n",
    "                           k_values=k_values, \n",
    "                           embedding=embedding, \n",
    "                           pos=pos, \n",
    "                           n=n)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"NB\":\n",
    "        results = tune_naive_bayes(df=df, \n",
    "                                   testing_ratio=testing_ratio, \n",
    "                                   vectorization_within_folds=vectorization_within_folds, \n",
    "                                   k_values=k_values,\n",
    "                                   embedding=embedding,\n",
    "                                   pos=pos,\n",
    "                                   n=n)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"RF\":\n",
    "        results = tune_random_forest(df=df, \n",
    "                                     testing_ratio=testing_ratio, \n",
    "                                     vectorization_within_folds=vectorization_within_folds, \n",
    "                                     k_values=k_values, \n",
    "                                     embedding=embedding,\n",
    "                                     pos=pos,\n",
    "                                     n=n)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "    \n",
    "    if show_class_accuracy:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Class names (assuming they are in the same order as in y_train or y_test)\n",
    "        class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "        \n",
    "        # Calculate per-class accuracy: TP / (TP + FN)\n",
    "        class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        # Print the accuracy for each class along with its name\n",
    "        for i, acc in enumerate(class_accuracies):\n",
    "            print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        class_names = np.unique(y_test)\n",
    "        \n",
    "        # Plotting the confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "        \n",
    "        # Label the axes\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47994ac-4c03-470e-bafc-fa807d6365b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f9cab-ec90-44eb-b925-44d1948f1525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7c8ca-0e4a-4160-8300-b42861621a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37684526-92da-4488-8bd6-4c6d7ccb7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bfbbe-0718-445f-86ba-cf7a9c942396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395b686-0ef1-4019-8d55-353aa5963b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db526a-26fa-4a39-aa12-661ba9d8a1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de796c-ff10-4632-8aec-447bbee7ced2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745bcaf-7805-48c7-929c-ce283f939417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4919272-9eed-4d64-b1c4-80069536deee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d568c3-79c8-4bcf-af1e-c93b741e93d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca3bfee",
   "metadata": {},
   "source": [
    "# Vector Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19aeb1",
   "metadata": {},
   "source": [
    "For now, I only test TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6485e28c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_clean\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Feature: text column\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df_clean[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Label: group column\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21265da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f31c73",
   "metadata": {},
   "source": [
    "**Now I test BERTje Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_bertje = df_clean.copy()\n",
    "\n",
    "# Load BERTje tokenizer and model\n",
    "MODEL_NAME = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels (convert string labels to integers)\n",
    "label_mapping = {label: idx for idx, label in enumerate(df_clean_bertje[\"group\"].unique())}\n",
    "df_clean_bertje[\"label\"] = df_clean_bertje[\"group\"].map(label_mapping)\n",
    "\n",
    "# Split dataset\n",
    "X_train_bertje, X_temp_bertje, y_train_bertje, y_temp_bertje = train_test_split(\n",
    "    df_clean_bertje[\"text\"].tolist(), df_clean_bertje[\"label\"].tolist(), test_size=0.3, random_state=42)\n",
    "X_val_bertje, X_test_bertje, y_val_bertje, y_test_bertje = train_test_split(X_temp_bertje, y_temp_bertje, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state[:, 0, :].squeeze().numpy()  # CLS token representation\n",
    "\n",
    "# Convert text data into embeddings\n",
    "train_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_train_bertje]).numpy()\n",
    "val_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_val_bertje]).numpy()\n",
    "test_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_test_bertje]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c10e",
   "metadata": {},
   "source": [
    "**Now I test Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7da54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f3b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62085a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23541cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6733448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfb2a12",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33550e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    val_score = classifier.score(X_val_tfidf, y_val)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad1401-d7c4-463a-bd21-198f67aeb3e4",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624cac5-4f4b-418b-936c-d0f610d3de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64904bff-0e5c-472f-98ed-cae003a0af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a99c80",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5be378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the SVM hyperparameters using the validation set\n",
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_tfidf, y_train)\n",
    "        val_score = classifier.score(X_val_tfidf, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95893b16-aeb5-4e10-97df-ee9bdb0886e5",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15571664-289b-4fe5-ab4d-51b322a1f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c05a32-11b7-4e41-9aab-22e89e376dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518a342-ca92-46e5-9c1b-2f0e4073b063",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1f5dc-cf03-4992-bbe6-5ba435f37616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Naive Bayes hyperparameters using the validation set\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Test different hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "\n",
    "for fit_prior_value in fit_prior_values:\n",
    "    for alpha in alpha_values:\n",
    "        classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "        classifier.fit(X_train_tfidf, y_train)\n",
    "        val_score = classifier.score(X_val_tfidf, y_val)\n",
    "        print(f\"Alpha: {alpha}, fit_prior: {fit_prior_value}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_alpha = alpha\n",
    "            best_fit_prior = fit_prior_value\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best alpha and fit_prior\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15bca6-f63d-418d-b5f2-165bbb05f774",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94617943-b6cb-4bb2-a169-3f91e644937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f2692-c486-40a8-8c82-306bca7b9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383bce3",
   "metadata": {},
   "source": [
    "# Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6759352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Hyperparameter tuning with LOO-CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for fit_prior_value in fit_prior_values:\n",
    "    for alpha in alpha_values:\n",
    "        scores = []\n",
    "        \n",
    "        for train_index, val_index in loo.split(X_train_tfidf):\n",
    "            X_train_cv, X_val = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "            y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "            \n",
    "            classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "            classifier.fit(X_train_cv, y_train_cv)\n",
    "            \n",
    "            y_pred = classifier.predict(X_val)\n",
    "            scores.append(accuracy_score(y_val, y_pred))\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"Alpha: {alpha}, fit_prior: {fit_prior_value}, LOO-CV Score: {mean_score}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_alpha = alpha\n",
    "            best_fit_prior = fit_prior_value\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best LOO-CV Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This leads to worse performance. Likely since Leave-one-out cross validation tends to create High-Variance Models.\n",
    "# Instead, I will use stratified K-fold cross validation. K will be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68115bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning with Stratified K-Fold CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_k = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "k_values = [2, 3, 5, 10, 20]  # Different values for K in StratifiedKFold\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for k in k_values:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fit_prior_value in fit_prior_values:\n",
    "        for alpha in alpha_values:\n",
    "            scores = []\n",
    "            \n",
    "            for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                X_train_cv, X_val = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                classifier.fit(X_train_cv, y_train_cv)\n",
    "                \n",
    "                y_pred = classifier.predict(X_val)\n",
    "                scores.append(accuracy_score(y_val, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_alpha = alpha\n",
    "                best_fit_prior = fit_prior_value\n",
    "                best_k = k\n",
    "\n",
    "print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5b144-6262-42e8-b9fd-1fadb2d7180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob\n",
    "\n",
    "for row in y_pred_prob:\n",
    "    formatted_row = [\"{:.4f}\".format(val) for val in row]\n",
    "    print(formatted_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86951f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, need to ensure TF-IDF vectorization happens within each fold to prevent leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af70a-6708-452c-891e-5a49be77aa5c",
   "metadata": {},
   "source": [
    "# TF-IDF vectorization within folds to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865cdd5-4051-4dfc-852c-6f2e38b953b3",
   "metadata": {},
   "source": [
    "Currently, vectorizations occurs over the full training set. <br>\n",
    "However, we train K models, 1 for each fold. <br>\n",
    "This means that for each fold, vectorizations should occur for the training data for that specific fold. <br>\n",
    "This avoids data leakage from our validation set to our training set. <br>\n",
    "Note that this is not strictly needed (since not separately within each fold is usually acceptable), but it should slightly improve performance at the cost of additional runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df_clean is already defined with 'text' and 'group' columns\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning with Stratified K-Fold CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_k = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "k_values = [2, 3, 5, 10, 20]  # Different values for K in StratifiedKFold\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for k in k_values:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fit_prior_value in fit_prior_values:\n",
    "        for alpha in alpha_values:\n",
    "            scores = []\n",
    "            \n",
    "            for train_index, val_index in skf.split(X_train, y_train):\n",
    "                # Split the raw text data for the current fold\n",
    "                X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                # Vectorize within the fold: fit on training fold, transform validation fold\n",
    "                vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "                \n",
    "                # Initialize and train the classifier\n",
    "                classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                \n",
    "                # Validate the model\n",
    "                y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_alpha = alpha\n",
    "                best_fit_prior = fit_prior_value\n",
    "                best_k = k\n",
    "\n",
    "print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "\n",
    "# Final model training on the entire training set using the best hyperparameters\n",
    "# Here, we fit the vectorizer on the full training set\n",
    "final_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform the test set using the vectorizer fitted on the entire training set\n",
    "X_test_tfidf = final_vectorizer.transform(X_test)\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13dc6b-4a39-4f94-bb7f-fd2bf7034bd6",
   "metadata": {},
   "source": [
    "# Turning model tuning into function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed27e7d-32c3-4907-a710-b238f98107eb",
   "metadata": {},
   "source": [
    "Since the process of tuning a classifier tends to not change much, we create a function for every type of classifier so that we can tune them without needing to re-write the code every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf67cc3-6266-4bbe-9b21-8a578bb374ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_random_forest(df: pd.DataFrame,\n",
    "                       testing_ratio: float = 0.15, \n",
    "                       vectorization_within_folds: bool = False,\n",
    "                       k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the Random Forest using stratified K-fold cross validation\n",
    "    best_depth = None\n",
    "    best_score = 0\n",
    "    depth_values = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "    # depth_values = [5, 10]  # For faster tests\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for depth in depth_values:\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_tfidf, y_train)\n",
    "                y_pred = classifier.predict(X_val_tfidf)\n",
    "                mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                print(f\"K: {k}, Depth: {depth}, Validation Accuracy: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_depth = depth\n",
    "                    best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for depth in depth_values:\n",
    "            scores = []\n",
    "\n",
    "            if vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train, y_train):\n",
    "                    # Split the raw text data for the current fold\n",
    "                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "\n",
    "            if not vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold)\n",
    "                    \n",
    "            scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Depth: {depth}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_depth = depth\n",
    "                best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best depth: {best_depth}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268e976-6eb6-413e-813f-a0b3818839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_SVM(df: pd.DataFrame,\n",
    "             testing_ratio: float = 0.15, \n",
    "             vectorization_within_folds: bool = False,\n",
    "             k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    best_score = 0\n",
    "    \n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]    \n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for C in C_values:\n",
    "                for kernel in kernels:\n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, C: {C}, Kernel: {kernel}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_C = C\n",
    "                        best_kernel = kernel\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for C in C_values:\n",
    "            for kernel in kernels:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, C: {C}, Kernel: {kernel}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_C = C\n",
    "                    best_kernel = kernel\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best C: {best_C}, Best kernel: {best_kernel}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d50d6c-bd14-4c2e-a16e-1680beb21c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_naive_bayes(df: pd.DataFrame,\n",
    "                     testing_ratio: float = 0.15, \n",
    "                     vectorization_within_folds: bool = False,\n",
    "                     k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    best_score = 0\n",
    "    \n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]  \n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                for fit_prior_value in fit_prior_values:\n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_alpha = alpha\n",
    "                        best_fit_prior = fit_prior_value\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            for fit_prior_value in fit_prior_values:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_alpha = alpha\n",
    "                    best_fit_prior = fit_prior_value\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86651e-dcd5-4cfa-a38d-9fd1f520ca81",
   "metadata": {},
   "source": [
    "# Overarching Model Selection Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0ab1c-1675-4995-8f8a-587f90a46875",
   "metadata": {},
   "source": [
    "Expected parameter values: <br>\n",
    "> - _classifier_: Selects the type of classifier from amongst the following: [\"SVM\", \"NB\", \"RF\"].\n",
    "> - _testing_ratio_: The ratio of the data that is reserved for testing. Any floating point in the inclusive interval [0, 1].\n",
    "> > Note that if $k=1$, the size of the validation set is assumed to be equal to the size of the testing set, specified by _testing_ratio_.\n",
    "> - _vectorization_within_folds_: Would you like to vectorize each individual fold rather than vectorizing the entire training set once? [True, False].\n",
    "> - _show_class_accuracy_: Would you like the accuracy per class to be displayed? [True, False].\n",
    "> - _show_confusion_matrix_: Would you like the resulting confusion matrix to be displayed? [True, False].\n",
    "> - _k_values_: All values of k which are tested for stratified k-fold cross validation. Any list containing only positive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcb640-3953-4609-aebd-4d46c44ce5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df: pd.DataFrame,\n",
    "                model_type: str = \"SVM\", \n",
    "                testing_ratio: float = 0.15, \n",
    "                vectorization_within_folds: bool = False, \n",
    "                show_class_accuracy: bool = True, \n",
    "                show_confusion_matrix: bool = True,\n",
    "                k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    print(f\"Tuning {model_type} classifier with a train/test split of {1-testing_ratio}/{testing_ratio} \\n\")\n",
    "    \n",
    "    #Raise appropriate error message in case of a faulty parameter value\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(f\"Invalid input data. Please ensure df is a Pandas DataFrame\")\n",
    "    if model_type not in [\"SVM\", \"NB\", \"RF\"]:\n",
    "        raise ValueError(f\"Invalid model_type. Choose from {'SVM', 'NB', 'RF'}\")\n",
    "    if testing_ratio < 0 or testing_ratio > 1:\n",
    "        raise ValueError(f\"Invalid testing ratio. Choose a value in the inclusive interval [0,1]\")\n",
    "    if type(vectorization_within_folds) != bool:\n",
    "        raise ValueError(f\"Invalid vectorization_within_folds value. Please ensure vectorization_within_folds is boolean\")\n",
    "    if type(show_class_accuracy) != bool:\n",
    "        raise ValueError(f\"Invalid show_class_accuracy value. Please ensure show_class_accuracy is boolean\")\n",
    "    if type(show_confusion_matrix) != bool:\n",
    "        raise ValueError(f\"Invalid show_confusion_matrix value. Please ensure show_confusion_matrix is boolean\")\n",
    "    if not all(isinstance(x, int) and x > 0 for x in k_values):\n",
    "        raise ValueError(f\"Invalid k_values. Please ensure all entries in k_values are positive integers\")\n",
    "    if 1 in k_values and vectorization_within_folds:\n",
    "        raise ValueError(f\"If k_values contains 1, vectorization_within_folds must be False since k=1 implies standard hold-out cross-validation, for which vectorization_within_folds must be False\")\n",
    "\n",
    "    model_mapping = {\"SVM\": SVC, \"NB\": MultinomialNB , \"RF\": RandomForestClassifier}\n",
    "    ModelClass = model_mapping[model_type]\n",
    "    \n",
    "    #Best classifier performance and number of stratified folds so far\n",
    "    best_score = 0\n",
    "    best_k = None\n",
    "\n",
    "    #RF hyperparameters:\n",
    "    best_depth = None\n",
    "    depth_values = [5, 10, 15, 20, 25, None]\n",
    "\n",
    "    #SVM hyperparameters:\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    kernel_values = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]\n",
    "\n",
    "    #NB hyperparameters:\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]\n",
    "    \n",
    "    if model_type == \"SVM\":\n",
    "        results = tune_SVM(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"NB\":\n",
    "        results = tune_naive_bayes(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"RF\":\n",
    "        results = tune_random_forest(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "    \n",
    "    if show_class_accuracy:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Class names (assuming they are in the same order as in y_train or y_test)\n",
    "        class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "        \n",
    "        # Calculate per-class accuracy: TP / (TP + FN)\n",
    "        class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        # Print the accuracy for each class along with its name\n",
    "        for i, acc in enumerate(class_accuracies):\n",
    "            print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        class_names = np.unique(y_test)\n",
    "        \n",
    "        # Plotting the confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "        \n",
    "        # Label the axes\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b44d4-078b-44ac-bc5c-10d0d16fcfe5",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95f2ca-af2d-44b3-bd2a-fad6ae19802c",
   "metadata": {},
   "source": [
    "The code below serves only to test the _train_model_ function and to detect and remove bugs. <br>\n",
    "The specific parameter values hold no significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501c74e-df1a-4709-b7b9-bc3980e153a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"RF\", \n",
    "            testing_ratio = 0.15, \n",
    "            vectorization_within_folds = False, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533dcd9-eb7c-4070-a617-7efc2b599430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"SVM\", \n",
    "            testing_ratio = 0.15, \n",
    "            vectorization_within_folds = False, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True,\n",
    "            k_values = [1,2,5,10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc203a32-b1c0-4056-85b0-0249736163ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"NB\", \n",
    "            testing_ratio = 0.2, \n",
    "            vectorization_within_folds = True, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True,\n",
    "           k_values = [2,12,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09ee41-232b-4c60-b3d4-4eeecec9d84e",
   "metadata": {},
   "source": [
    "# Showing percentages per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b28023-60c5-4177-bdf9-68b3beb1b03f",
   "metadata": {},
   "source": [
    "First for SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5367b-b2c1-4571-8c00-5d963176d955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bbaae6a-846e-455b-92ce-03b30565214d",
   "metadata": {},
   "source": [
    "Now for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9a95f-b1d1-4c33-b06c-e26ea2cb0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdd1f8-26ca-42d5-a691-df03481a7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: i have noticed that test performance tends to be higher for k>1 even though the validation score would suggest that k=1 is best. \n",
    "#I think this is because stratified cross validation generalizes better.\n",
    "#How do I decide on k? I can't run everything over test set, that would turn test set into 2nd validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e394536-81e2-4f6b-91e4-fe62c4b7cc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc7ed40",
   "metadata": {},
   "source": [
    "# Testing BERTje word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48c9e1",
   "metadata": {},
   "source": [
    "**NOTE**: These are just basic tests to see if the word embeddings hold potential. <br>\n",
    "The option to use word embeddings should be added to the train_model function above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca26669",
   "metadata": {},
   "source": [
    "First we test random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be60e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(train_embeddings, y_train_bertje)\n",
    "    val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acdd27",
   "metadata": {},
   "source": [
    "Now we Test SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(train_embeddings, y_train_bertje)\n",
    "        val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef5695",
   "metadata": {},
   "source": [
    "Now we test Naive Bayes (We test GaussianNB, since MultinomialNB does not work for continuous features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6240373",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_var_smoothing = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Test different hyperparameter values\n",
    "var_smoothing_values = [10**-11, 10**-10, 10**-9, 10**-8, 10**-7]\n",
    "\n",
    "for var_smoothing in var_smoothing_values:\n",
    "    classifier = GaussianNB(var_smoothing=var_smoothing)\n",
    "    classifier.fit(train_embeddings, y_train_bertje)\n",
    "    val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "    print(f\"Var_smoothing: {var_smoothing}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_var_smoothing = var_smoothing\n",
    "\n",
    "print(f\"\\nBest var_smoothing: {best_var_smoothing}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best alpha and fit_prior\n",
    "final_classifier = GaussianNB(var_smoothing=best_var_smoothing)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28f230",
   "metadata": {},
   "source": [
    "Surprisingly, these models all perform worse than their tf-idf counterparts. <br>\n",
    "**TO DO**: Rename train_embeddings, val_embeddings, test_embeddings to X_train_bertje, X_val_bertje, X_test_bertje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2ae40",
   "metadata": {},
   "source": [
    "**Idea**: Once pos_tags are used for classification, test and compare performance for:\n",
    "> - Bag of Words <br>\n",
    "> - TF-IDF <br>\n",
    "> - BERTje <br>\n",
    "> - mBERT (Multilingual BERT) <br>\n",
    "> - RobBERT (Dutch RoBERTa model)\n",
    "> - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
