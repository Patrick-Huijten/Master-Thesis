{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc6c140",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d174e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF \n",
    "# !pip install transformers torch\n",
    "# !pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a64d74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "import logging\n",
    "import spacy\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
    "from scipy.sparse import hstack\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel, TFAutoModel\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b699963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b417a19",
   "metadata": {},
   "source": [
    "# Loading Text From PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab59a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ook bij kredietverlening aan het mkb zijn bankpraktijken al jaren \n",
      "onacceptabel, en andere lezersreacties \n",
      "De lezers van het FD reageren deze week op het discutabele handelen van banken, hoe ‘Made in \n",
      "Germany’ verdwijnt, de bittere bijsmaak van graan en een goed getimed rapport. \n",
      " \n",
      "Vermeende nutsfunctie \n",
      "De hoogleraren Arnoud Boot en Harald Benink vragen terecht aandacht voor het discutabele \n",
      "optreden van banken (FD, 25 september). Ook bij de kredietverlening aan het mkb zijn bankpraktijken \n",
      "al jaren maatschappelijk onacceptabel. Zij verlenen geen krediet meer aan bepaalde sectoren, het \n",
      "serviceniveau daalde sterk, de adviesfunctie ontbreekt en zij vragen enorme zekerheden. \n",
      "Terwijl de banken zelf zo min mogelijk eigen vermogen willen aanhouden, worden bij bestaande \n",
      "klanten de duimschroeven snel aangedraaid als de liquiditeit krapper wordt. Niet omdat banken risico \n",
      "dragen, want er zijn immers panden, voorraden en andere tegoeden. Het draait louter om het \n",
      "winstgedreven verdienmodel van banken, dat is losgezongen van maatschappelijke \n",
      "verantwoordelijkheden. \n",
      "Banken maken misbruik van het vertrouwen dat zij van oudsher genieten. Tot zo’n twintig of dertig \n",
      "jaar geleden betrof bankieren nog een algemene nutsfunctie. Deze is in de neoliberale dynamiek \n",
      "verdwenen. Illustratief is de recente koerswijziging van de Rabobank, die zijn coöperatieve structuur \n",
      "nu achter zich laat. In reclame-uitingen verwijzen ze er nog wel naar, maar dat zou je misleiding \n",
      "kunnen noemen. \n",
      "Zoals Boot en Benink stellen, moet de politiek kaders scheppen waarbinnen banken de balans tussen \n",
      "winst en nutsfunctie kunnen hervinden. \n",
      " \n",
      "Made in Germany \n",
      "Sebastien Valkenberg vergeet in zijn column over de Duitse auto-industrie iets te vermelden (FD, 26 \n",
      "september). De auto-industrie bij onze oosterburen verzette niet op tijd de bakens richting e-\n",
      "mobiliteit en goedkopere productiemethodes. Nu moet de Duitse overheid bijspringen om banen te \n",
      "redden die hoe dan ook op termijn verdwijnen, en nu al nauwelijks zijn in te vullen. \n",
      "De vergelijking met het lot van de Nederlandse scheepsbouw dringt zich op. Blijven groeien tegen \n",
      "beter weten in, of nemen we afscheid van sectoren die aan het einde van hun levensduur geraken? \n",
      "Dat is geen degrowth, zoals Valkenberg schrijft, maar noodzakelijke new growth. \n",
      " \n",
      "Bittere bijsmaak \n",
      "Bartjens besluit een column over graanhandelshuis Louis Dreyfus met een geestige beeldspraak (FD, \n",
      "24 september): ‘En zelfs de Nederlandse belastingbetaler pikt via de brievenbus een graantje mee.’ \n",
      "Dit krijgt een bittere bijsmaak als je beseft dat, terwijl grote graanverhandelende bedrijven \n",
      "miljardenwinsten maken, wereldwijd 800 miljoen mensen een acuut tekort hebben aan graan en \n",
      "ander basisvoedsel, en elke dag met honger naar bed gaan. \n",
      "Noodhulpverlenende instanties, zoals het Wereldvoedselprogramma van de Verenigde Naties, \n",
      "moeten bedelen bij rijke overheden om het snel toenemend aantal noden in de wereld te lenigen. \n",
      "Afgelopen jaar was er een historisch tekort van 60% (€14,5 mrd) om alle hongersnoden in de wereld \n",
      "te bestrijden. \n",
      "Megawinsten bij Dreyfus en consorten zijn immoreel en moeten door overheden – en zeker door \n",
      "belastingparadijs Nederland – flink méér belast worden. De opbrengst daarvan kan zo naar het \n",
      "Wereldvoedselprogramma. \n",
      " \n",
      "Pijnlijk proces \n",
      "Net als eerder bij het rapport van Mario Draghi, trek ik bij het lezen van het rapport van de \n",
      "Wetenschappelijke Raad voor het Regeringsbeleid (WRR) eenzelfde conclusie: de diagnose is goed, \n",
      "maar ik twijfel of de juiste kuur wordt voorgeschreven (FD, 25 september). \n",
      "De conclusies van het WRR-rapport zijn eerlijk gezegd niet nieuw; er staat weinig in wat we niet al \n",
      "wisten. We weten immers al jaren dat alle eurolanden – met uitzondering van Nederland – hun \n",
      "pensioenen door middel van een omslagstelsel financieren, én dat dit stelsel een tikkende tijdbom is. \n",
      "Vergrijzing is niet van gisteren. \n",
      "Dus het is naar mijn idee geen toeval dat de WRR, precies een week na publicatie van het Draghi-\n",
      "rapport, met de aanbeveling komt om Eurobonds wél goed te keuren, onder de voorwaarde dat \n",
      "andere landen hun oudedagsvoorziening herstructureren. Dit is echter het paard achter de wagen \n",
      "spannen. Als Nederland zijnde geven we daarmee iets uit handen, waar we niet veel meer dan een \n",
      "vage belofte voor terugkrijgen – zeker omdat iedere begrotingsafspraak in de Eurozone de afgelopen \n",
      "25 jaar is geschonden. \n",
      "Het bedrijven van industriepolitiek dient vooral de grote EU-landen, niet de kleinere. Het wederom \n",
      "aanzetten van de geldpers schikt met name de zuidelijke eurolanden, en leidt op lange termijn tot \n",
      "inflatie – wat wenselijker is voor de zuidelijke eurolanden dan voor Nederland. Zo komt er een \n",
      "moment dat Nederland zichzelf een zeer ongemakkelijke vraag moet stellen: hebben we nog wel baat \n",
      "bij de euro? Nemen we niet te klakkeloos aan dat een land met een open economie baat heeft bij een \n",
      "gemeenschappelijke munt? Landen als Zweden en Denemarken zijn immers wél lid van de Europese \n",
      "Unie, doen niet mee met de euro maar scoren wél goed op economische toplijstjes. \n",
      "Draghi stelt dat wanneer de EU niet meer gezamenlijk gaat investeren, onze welvaartstaat in gevaar \n",
      "komt. Mijn stelling is dat juist deze welvaartstaat een onderdeel van het probleem is in Europa, dat te \n",
      "weinig ambitie en te veel sociale zekerheid kent. Dus: hervorm de arbeidsmarkt, de sociale zekerheid, \n",
      "het pensioenstelsel en versterk de interne markt. Dit is een pijnlijk proces, met mogelijk veel ‘gele \n",
      "hesjes’ tot gevolg, maar soms moet je een stap terug doen om twee stappen vooruit te kunnen \n",
      "zetten. \n",
      "Ik ben voorstander van investeren, maar laten we eerst gebruikmaken van de bestaande potjes. \n",
      "NextGenerationEU, de eerste generatie eurobonds, is bijvoorbeeld goedgekeurd op basis van \n",
      "eenmaligheid en tijdelijkheid. Dat men nu permanente eurobonds oppert, ligt natuurlijk in de lijn der \n",
      "verwachting. Maar om die gezamenlijke aangegane schulden ooit terug te betalen, zullen eurobonds \n",
      "impliciet ook leiden tot een verdere overdracht van de nationale soevereiniteit aan Brussel op het \n",
      "gebied van de financiën. \n",
      " \n",
      "Verlaat pensioen \n",
      "De discussie over een vervroegd pensioen voor werknemers in zware beroepen moet worden \n",
      "verbreed (FD, 23 september). Ook werknemers die, in lichtere beroepen, langer willen doorwerken \n",
      "moeten een serieuze kans krijgen. \n",
      "Alle werknemers die, in de jaren voor hun AOW-leeftijd, bewijzen hun werk naar volle tevredenheid \n",
      "te verrichten, moeten de kans krijgen met dat werk door te gaan. Op deze wijze bestrijden we het \n",
      "tekort aan werknemers. \n",
      "Werkgevers moeten zich daarbij inspannen om oudere werknemers op verantwoorde wijze langer te \n",
      "laten doorwerken. Hier moeten overheid en bedrijfsleven afspraken over maken met de vakbonden. \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"test article, zakelijke dienstverlening\\\\test_article_manually_extracted.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8be373",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2a8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ensure that only pdf files of articles are present in the subfolders of the specified directory\n",
      "26 article(s) detected in Bouw & Vastgoed folder\n",
      "37 article(s) detected in Handel & Industrie folder\n",
      "37 article(s) detected in Zakelijke Dienstverlening folder\n",
      "20 article(s) detected in Zorg folder\n"
     ]
    }
   ],
   "source": [
    "directory = 'data'\n",
    "df = pd.DataFrame(columns=['article_id' , 'paragraph_id', 'text', 'group', 'publication_date'])\n",
    "\n",
    "print('Please ensure that only pdf files of articles are present in the subfolders of the specified directory')\n",
    "article_nr = 1\n",
    "for folder in os.listdir(directory):\n",
    "    folder_size = len(os.listdir(directory + \"\\\\\" + folder))\n",
    "    print(f'{folder_size} article(s) detected in {folder} folder')\n",
    "    \n",
    "    for article in os.listdir(directory + '\\\\' + folder):\n",
    "        text = extract_text_from_pdf(directory + '\\\\' + folder + '\\\\' + article)\n",
    "        date = article.split(' ')[-1].split('.')[0] #Remove the article number and \".pdf\" to obtain the publication date\n",
    "        \n",
    "        paragraphs = [para.strip() for para in text.split(\"\\n \\n\") if para.strip()]\n",
    "        para_nr = 1\n",
    "        for para in paragraphs:\n",
    "            df_temp = pd.DataFrame([[article_nr, para_nr, para, folder, date]], \n",
    "                                   columns=['article_id' , 'paragraph_id', 'text', 'group', 'publication_date'])\n",
    "            df = pd.concat([df, df_temp])\n",
    "            para_nr += 1\n",
    "        article_nr += 1\n",
    "        \n",
    "df.set_index(['article_id' , 'paragraph_id'], inplace=True)\n",
    "df['publication_date'] = pd.to_datetime(df['publication_date'], format='%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3333cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>group</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>Provincies willen aan de slag met versoepeling...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>2024-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Het draait allemaal om de drempelwaarde voor e...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>2024-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Met een hogere drempelwaarde zouden minder ver...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>2024-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In het hoofdlijnenakkoord hebben de vier coali...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>2024-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De ondergrens is al langer onderwerp van discu...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>2024-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>2</th>\n",
       "      <td>Telgenkamp vestigt haar hoop voor de korte ter...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>2024-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <th>1</th>\n",
       "      <td>Waarom verzekeraars inkomsten uit zwart werk w...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>2024-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">120</th>\n",
       "      <th>1</th>\n",
       "      <td>Verzekeraar wil klant helpen met zorgbemiddeli...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>2024-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verzekeraar wil wachtende patiënt aan snelle z...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>2024-10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zorgbemiddeling is geen wondermiddel, maar kan...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>2024-10-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             Provincies willen aan de slag met versoepeling...   \n",
       "           2             Het draait allemaal om de drempelwaarde voor e...   \n",
       "           3             Met een hogere drempelwaarde zouden minder ver...   \n",
       "           4             In het hoofdlijnenakkoord hebben de vier coali...   \n",
       "           5             De ondergrens is al langer onderwerp van discu...   \n",
       "...                                                                    ...   \n",
       "118        2             Telgenkamp vestigt haar hoop voor de korte ter...   \n",
       "119        1             Waarom verzekeraars inkomsten uit zwart werk w...   \n",
       "120        1             Verzekeraar wil klant helpen met zorgbemiddeli...   \n",
       "           2             Verzekeraar wil wachtende patiënt aan snelle z...   \n",
       "           3             Zorgbemiddeling is geen wondermiddel, maar kan...   \n",
       "\n",
       "                                   group publication_date  \n",
       "article_id paragraph_id                                    \n",
       "1          1             Bouw & Vastgoed       2024-08-28  \n",
       "           2             Bouw & Vastgoed       2024-08-28  \n",
       "           3             Bouw & Vastgoed       2024-08-28  \n",
       "           4             Bouw & Vastgoed       2024-08-28  \n",
       "           5             Bouw & Vastgoed       2024-08-28  \n",
       "...                                  ...              ...  \n",
       "118        2                        Zorg       2024-10-08  \n",
       "119        1                        Zorg       2024-10-17  \n",
       "120        1                        Zorg       2024-10-16  \n",
       "           2                        Zorg       2024-10-16  \n",
       "           3                        Zorg       2024-10-16  \n",
       "\n",
       "[374 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21dfca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 paragraphs in Bouw & Vastgoed.\n",
      "114 paragraphs in Handel & Industrie.\n",
      "83 paragraphs in Zakelijke Dienstverlening.\n",
      "53 paragraphs in Zorg.\n"
     ]
    }
   ],
   "source": [
    "for group in df['group'].unique():\n",
    "    print(f\"{len(df[df['group'] == group])} paragraphs in {group}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f97bc5",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270d6f4",
   "metadata": {},
   "source": [
    "Firstly, we load the nl_core_news_sm model and specify that [NEWLINE] should be treated as a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c278bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Dutch POS model by uncommenting the line below\n",
    "\n",
    "# !python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fda5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'nl_core_news_sm' model\n",
    "nlp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "# Add [NEWLINE] as a single token so that it is not split into 3 seperate tokens\n",
    "special_cases = {\"[NEWLINE]\": [{\"ORTH\": \"[NEWLINE]\"}]}\n",
    "nlp.tokenizer.add_special_case(\"[NEWLINE]\", [{\"ORTH\": \"[NEWLINE]\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a8bb9",
   "metadata": {},
   "source": [
    "A pre-processed dataset df_clean is constructed out of df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "792dae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "df_clean['original_text'] = df_clean['text'].copy()\n",
    "df_clean = df_clean[['original_text', 'text', 'group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04eef4",
   "metadata": {},
   "source": [
    "**Case Normalization**: <br>\n",
    "- Lowercasing\n",
    "- Replacing \\n with '[NEWLINE] ' \n",
    "- Removing duplicate spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998fbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_normalization(text):\n",
    "    \"\"\"Returns string of input containing only lowercase letters apart from [NEWLINE], which replaces \\n\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' [NEWLINE] ')\n",
    "    while text != text.replace('  ', ' '):\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text\n",
    "\n",
    "df_clean['text'] = df_clean['text'].apply(case_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2fb0e",
   "metadata": {},
   "source": [
    "**Punctuation Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab0eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"Returns the input text with all punctuation removed\"\"\"\n",
    "    \n",
    "    text = text.translate(text.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.replace(\"NEWLINE\", \"[NEWLINE]\")\n",
    "    return text\n",
    "\n",
    "df_clean['text'] = df_clean['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4b17b",
   "metadata": {},
   "source": [
    "**Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bb244",
   "metadata": {},
   "source": [
    "Remove words that do not add semantic meaning to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12591f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snelle bruine vos springt luie hond .\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"De snelle bruine vos springt over de luie hond.\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter out stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Join the filtered words back into a single string\n",
    "text = \" \".join(filtered_words)\n",
    "\n",
    "# Print the result\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69d5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"Returns string of input text with stopwords removed\"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    text = \" \".join(filtered_words)\n",
    "    return text\n",
    "    \n",
    "    \n",
    "# nlp = spacy.load(\"nl_core_news_sm\")\n",
    "df_clean['text'] = df_clean['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a0a13",
   "metadata": {},
   "source": [
    "**POS Tagging**. <br>\n",
    "<br>\n",
    "There are 2 types of POS tagging: <br>\n",
    "- Rule-based POS tagging\n",
    "- Statistical POS tagging \n",
    "\n",
    "**Benefits** of **rule-based** Part-of-speech (POS) tagging:\n",
    "- Simple to implement and understand\n",
    "- It doesn’t require a lot of computational resources or training data\n",
    "- It can be easily customized to specific domains or languages\n",
    "\n",
    "**Disadvantages** of **rule-based** Part-of-speech (POS) tagging:\n",
    "- Less accurate than statistical taggers\n",
    "- Limited by the quality and coverage of the rules\n",
    "- It can be difficult to maintain and update\n",
    "\n",
    "**Benefits** of **Statistical** Part-of-speech (POS) Tagging:\n",
    "- More accurate than rule-based taggers\n",
    "- Don’t require a lot of human-written rules\n",
    "- Can learn from large amounts of training data\n",
    "\n",
    "**Disadvantages** of **statistical** Part-of-speech (POS) Tagging:\n",
    "- Requires more computational resources and training data\n",
    "- It can be difficult to interpret and debug\n",
    "- Can be sensitive to the quality and diversity of the training data\n",
    "\n",
    "I select Statistical POS tagging since the accuracy tends to be higher and since pre-trained POS-models are avilable, the requirement for a lot of training data is no problem. Additionally, the required computational power is no problem due to the small size of the used data for this project. <br>\n",
    "For more information on the used model, see https://github.com/evanmiltenburg/Dutch-tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6cf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a sentence\n",
    "# text = \"Ik ben een student\"\n",
    "\n",
    "# # Process the sentence using spaCy's NLP pipeline\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Iterate through the token and print the token text and POS tag\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba201831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def POS_tagging(text):\n",
    "#     \"\"\"Returns a list of (token, POS tag) tuples for the input text\"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "#     return pos_tags\n",
    "\n",
    "# df_clean['pos_tags'] = df_clean['text'].apply(POS_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c506c41",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a59f69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          PoS            Lemma          \n",
      "---------------------------------------------\n",
      "De             DET            de             \n",
      "katten         NOUN           kat            \n",
      "liepen         VERB           liepen         \n",
      "in             ADP            in             \n",
      "de             DET            de             \n",
      "tuin           NOUN           tuin           \n",
      ".              PUNCT          .              \n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"De katten liepen in de tuin.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print PoS tagging and Lemmatization for each token\n",
    "print(f\"{'Token':<15}{'PoS':<15}{'Lemma':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15}{token.pos_:<15}{token.lemma_:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f49be28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provincies NOUN provincie\n",
      "\n",
      "willen VERB willen\n",
      "\n",
      "slag NOUN slag\n",
      "\n",
      "versoepeling NOUN versoepeling\n",
      "\n",
      "stikstofregels NOUN stikstofregel\n",
      "\n",
      "[NEWLINE] SYM [NEWLINE]\n",
      "\n",
      "kabinet NOUN kabinet\n",
      "\n",
      "beoogde VERB beoogen\n",
      "\n",
      "hogere ADJ hoog\n",
      "\n",
      "drempelwaarde NOUN drempelwaarde\n",
      "\n",
      "lijkt VERB lijken\n",
      "\n",
      "rapport NOUN rapport\n",
      "\n",
      "[NEWLINE] PRON [NEWLINE]\n",
      "\n",
      "provincies NOUN provincie\n",
      "\n",
      "willen VERB willen\n",
      "\n",
      "‘ PUNCT ‘\n",
      "\n",
      "voortvarend VERB voortvaren\n",
      "\n",
      "’ NUM ’\n",
      "\n",
      "slag NOUN slag\n",
      "\n",
      "versoepeling NOUN versoepeling\n",
      "\n",
      "stikstofregels NOUN stikstofregel\n",
      "\n",
      "[NEWLINE] SYM [NEWLINE]\n",
      "\n",
      "waarmee ADV waarmee\n",
      "\n",
      "nieuwe ADJ nieuw\n",
      "\n",
      "kabinet NOUN kabinet\n",
      "\n",
      "nederland PROPN Nederland\n",
      "\n",
      "slot NOUN slot\n",
      "\n",
      "krijgen VERB krijgen\n",
      "\n",
      "aannemelijk ADJ aannemelijk\n",
      "\n",
      "[NEWLINE] SYM [NEWLINE]\n",
      "\n",
      "belangrijke ADJ belangrijk\n",
      "\n",
      "horde ADJ horde\n",
      "\n",
      "stikstofcrisis ADJ stikstofcrisis\n",
      "\n",
      "groot ADJ groot\n",
      "\n",
      "aangenomen VERB aannemen\n",
      "\n",
      "oordelen NOUN oordelen\n",
      "\n",
      "[NEWLINE] SYM [NEWLINE]\n",
      "\n",
      "wetenschappers NOUN wetenschapper\n",
      "\n",
      "tno PRON tno\n",
      "\n",
      "universiteit NOUN universiteit\n",
      "\n",
      "amsterdam PROPN Amsterdam\n",
      "\n",
      "onderzoek NOUN onderzoek\n",
      "\n",
      "opdracht NOUN opdracht\n",
      "\n",
      "[NEWLINE] VERB [NEWLINE]\n",
      "\n",
      "interprovinciaal VERB interprovinciaal\n",
      "\n",
      "overleg NOUN overleg\n",
      "\n",
      "ipo NOUN ipo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df_clean.loc[(1,1),'text'])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09abcd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>group</th>\n",
       "      <th>text before lemmatization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>Provincies willen aan de slag met versoepeling...</td>\n",
       "      <td>provincie willen slag versoepeling stikstofreg...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>provincies willen slag versoepeling stikstofre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Het draait allemaal om de drempelwaarde voor e...</td>\n",
       "      <td>draaien allemaal drempelwaran stikstofvergunni...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>draait allemaal drempelwaarde stikstofvergunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Met een hogere drempelwaarde zouden minder ver...</td>\n",
       "      <td>hoog drempelwaard vergunning [NEWLINE] aangevo...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hogere drempelwaarde vergunningen [NEWLINE] aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In het hoofdlijnenakkoord hebben de vier coali...</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartij afsprek...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartijen afges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De ondergrens is al langer onderwerp van discu...</td>\n",
       "      <td>ondergren lang onderwerp discussie huidig Nede...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>ondergrens langer onderwerp discussie huidige ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>2</th>\n",
       "      <td>Telgenkamp vestigt haar hoop voor de korte ter...</td>\n",
       "      <td>telgenkamp vestigen hoop kort termijn twee cru...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>telgenkamp vestigt hoop korte termijn twee cru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <th>1</th>\n",
       "      <td>Waarom verzekeraars inkomsten uit zwart werk w...</td>\n",
       "      <td>verzekeraar inkomst zwart werk vergoeden [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraars inkomsten zwart werk vergoeden [N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">120</th>\n",
       "      <th>1</th>\n",
       "      <td>Verzekeraar wil klant helpen met zorgbemiddeli...</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verzekeraar wil wachtende patiënt aan snelle z...</td>\n",
       "      <td>verzekeraar wachten patiënt snel zorg helpen [...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar wachtende patiënt snelle zorg help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zorgbemiddeling is geen wondermiddel, maar kan...</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zeggen Haa...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zegt haarl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             original_text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             Provincies willen aan de slag met versoepeling...   \n",
       "           2             Het draait allemaal om de drempelwaarde voor e...   \n",
       "           3             Met een hogere drempelwaarde zouden minder ver...   \n",
       "           4             In het hoofdlijnenakkoord hebben de vier coali...   \n",
       "           5             De ondergrens is al langer onderwerp van discu...   \n",
       "...                                                                    ...   \n",
       "118        2             Telgenkamp vestigt haar hoop voor de korte ter...   \n",
       "119        1             Waarom verzekeraars inkomsten uit zwart werk w...   \n",
       "120        1             Verzekeraar wil klant helpen met zorgbemiddeli...   \n",
       "           2             Verzekeraar wil wachtende patiënt aan snelle z...   \n",
       "           3             Zorgbemiddeling is geen wondermiddel, maar kan...   \n",
       "\n",
       "                                                                      text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincie willen slag versoepeling stikstofreg...   \n",
       "           2             draaien allemaal drempelwaran stikstofvergunni...   \n",
       "           3             hoog drempelwaard vergunning [NEWLINE] aangevo...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartij afsprek...   \n",
       "           5             ondergren lang onderwerp discussie huidig Nede...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigen hoop kort termijn twee cru...   \n",
       "119        1             verzekeraar inkomst zwart werk vergoeden [NEWL...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachten patiënt snel zorg helpen [...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zeggen Haa...   \n",
       "\n",
       "                                   group  \\\n",
       "article_id paragraph_id                    \n",
       "1          1             Bouw & Vastgoed   \n",
       "           2             Bouw & Vastgoed   \n",
       "           3             Bouw & Vastgoed   \n",
       "           4             Bouw & Vastgoed   \n",
       "           5             Bouw & Vastgoed   \n",
       "...                                  ...   \n",
       "118        2                        Zorg   \n",
       "119        1                        Zorg   \n",
       "120        1                        Zorg   \n",
       "           2                        Zorg   \n",
       "           3                        Zorg   \n",
       "\n",
       "                                                 text before lemmatization  \n",
       "article_id paragraph_id                                                     \n",
       "1          1             provincies willen slag versoepeling stikstofre...  \n",
       "           2             draait allemaal drempelwaarde stikstofvergunni...  \n",
       "           3             hogere drempelwaarde vergunningen [NEWLINE] aa...  \n",
       "           4             hoofdlijnenakkoord vier coalitiepartijen afges...  \n",
       "           5             ondergrens langer onderwerp discussie huidige ...  \n",
       "...                                                                    ...  \n",
       "118        2             telgenkamp vestigt hoop korte termijn twee cru...  \n",
       "119        1             verzekeraars inkomsten zwart werk vergoeden [N...  \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...  \n",
       "           2             verzekeraar wachtende patiënt snelle zorg help...  \n",
       "           3             zorgbemiddeling wondermiddel helpen zegt haarl...  \n",
       "\n",
       "[374 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatization(df, text_column=\"text\", output_column=\"text\"):\n",
    "    \"\"\"Lemmatizes the text in a specified column of a DataFrame and adds the results to a new column.\"\"\"\n",
    "    \n",
    "    # Ensure the input column exists in the DataFrame\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' does not exist in the DataFrame.\")\n",
    "        \n",
    "    # Apply SpaCy processing and lemmatization\n",
    "    df[output_column] = df[text_column].apply(\n",
    "        lambda text: \" \".join([token.lemma_ for token in nlp(text) if not token.is_punct and not token.is_space]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_clean['text before lemmatization'] = df_clean['text'].copy()\n",
    "df_clean = lemmatization(df_clean, text_column=\"text\")\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf7d8c",
   "metadata": {},
   "source": [
    "**POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17266f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(text):\n",
    "    \"\"\"Returns a list of (token, POS tag) tuples for the input text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "df_clean['pos_tags'] = df_clean['text'].apply(POS_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655e454",
   "metadata": {},
   "source": [
    "# Testing for POS and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed2f32",
   "metadata": {},
   "source": [
    "Test if performance increases when POS tags are explicitely used to reinforce lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9e7d0",
   "metadata": {},
   "source": [
    "**NOTE:** POS-tagging occurs twice in the pre-processing: Once before lemmatization and once after. The first POS-tagging results are used to reinforce the lemmatization by providing more detailed input. After Lemmatization, POS-tagging are once again obtained to ensure that the final POS-tags match the final text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07a58cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_experimental = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebc46cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "123c2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute the 2 lines below to install the required nltk files, which only needs to be done once.\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f131350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90547b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert spaCy POS to WordNet POS (needed for accurate lemmatization)\n",
    "def spacy_to_wordnet_pos(spacy_pos):\n",
    "    if spacy_pos.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif spacy_pos.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif spacy_pos.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif spacy_pos.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9eceefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pos(pos_tags):\n",
    "    return \" \".join([lemmatizer.lemmatize(word, spacy_to_wordnet_pos(pos)) for word, pos in pos_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed25b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags_after_lemmatization(lemmatized_text):\n",
    "    doc = nlp(lemmatized_text)\n",
    "    return [(token.text, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d2aba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id  paragraph_id\n",
      "1           1               [(provincies, NOUN), (willen, VERB), (slag, NO...\n",
      "            2               [(draait, VERB), (allemaal, ADV), (drempelwaar...\n",
      "            3               [(hogere, ADJ), (drempelwaarde, VERB), (vergun...\n",
      "            4               [(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...\n",
      "            5               [(ondergrens, NOUN), (langer, ADJ), (onderwerp...\n",
      "                                                  ...                        \n",
      "118         2               [(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...\n",
      "119         1               [(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...\n",
      "120         1               [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...\n",
      "            2               [(verzekeraar, ADJ), (wachtende, VERB), (patië...\n",
      "            3               [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...\n",
      "Name: pos_tags_before_lemmatization, Length: 374, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_clean_experimental['pos_tags_before_lemmatization'] = df_clean_experimental['text before lemmatization'].apply(POS_tagging)\n",
    "print(df_clean_experimental['pos_tags_before_lemmatization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcb682e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id  paragraph_id\n",
      "1           1               provincies willen slag versoepeling stikstofre...\n",
      "            2               draait allemaal drempelwaarde stikstofvergunni...\n",
      "            3               hogere drempelwaarde vergunningen [NEWLINE] aa...\n",
      "            4               hoofdlijnenakkoord vier coalitiepartijen afges...\n",
      "            5               ondergrens langer onderwerp discussie huidige ...\n",
      "                                                  ...                        \n",
      "118         2               telgenkamp vestigt hoop korte termijn twee cru...\n",
      "119         1               verzekeraars inkomsten zwart werk vergoeden [N...\n",
      "120         1               verzekeraar klant helpen zorgbemiddeling [NEWL...\n",
      "            2               verzekeraar wachtende patiënt snelle zorg help...\n",
      "            3               zorgbemiddeling wondermiddel helpen zegt haarl...\n",
      "Name: text, Length: 374, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_clean_experimental['text'] = df_clean_experimental['pos_tags_before_lemmatization'].apply(lemmatize_with_pos)\n",
    "print(df_clean_experimental['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e891f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id  paragraph_id\n",
      "1           1               [(provincies, NOUN), (willen, VERB), (slag, NO...\n",
      "            2               [(draait, VERB), (allemaal, ADV), (drempelwaar...\n",
      "            3               [(hogere, ADJ), (drempelwaarde, VERB), (vergun...\n",
      "            4               [(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...\n",
      "            5               [(ondergrens, NOUN), (langer, ADJ), (onderwerp...\n",
      "                                                  ...                        \n",
      "118         2               [(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...\n",
      "119         1               [(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...\n",
      "120         1               [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...\n",
      "            2               [(verzekeraar, ADJ), (wachtende, VERB), (patië...\n",
      "            3               [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...\n",
      "Name: pos_tags_after_lemmatization, Length: 374, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_clean_experimental['pos_tags_after_lemmatization'] = df_clean_experimental['text'].apply(get_pos_tags_after_lemmatization)\n",
    "print(df_clean_experimental['pos_tags_after_lemmatization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3047ee2",
   "metadata": {},
   "source": [
    "**POS Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now, we do not implement this since it clashes with NER. \n",
    "#If we need to reduce the dimension of the data, we can implement it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78962f7d",
   "metadata": {},
   "source": [
    "**Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80703342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#May overcomplicate the data, check if it improves performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11176e48",
   "metadata": {},
   "source": [
    "**NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94358926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50e27dbc",
   "metadata": {},
   "source": [
    "**Stemming** (Altenative for Lemmatization. Check when this should be done in the pre-processing pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78681d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "661d2fb0",
   "metadata": {},
   "source": [
    "**Resulting DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cc2ddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>group</th>\n",
       "      <th>text before lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>Provincies willen aan de slag met versoepeling...</td>\n",
       "      <td>provincie willen slag versoepeling stikstofreg...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>provincies willen slag versoepeling stikstofre...</td>\n",
       "      <td>[(provincie, NOUN), (willen, VERB), (slag, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Het draait allemaal om de drempelwaarde voor e...</td>\n",
       "      <td>draaien allemaal drempelwaran stikstofvergunni...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>draait allemaal drempelwaarde stikstofvergunni...</td>\n",
       "      <td>[(draaien, VERB), (allemaal, ADV), (drempelwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Met een hogere drempelwaarde zouden minder ver...</td>\n",
       "      <td>hoog drempelwaard vergunning [NEWLINE] aangevo...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hogere drempelwaarde vergunningen [NEWLINE] aa...</td>\n",
       "      <td>[(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In het hoofdlijnenakkoord hebben de vier coali...</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartij afsprek...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartijen afges...</td>\n",
       "      <td>[(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De ondergrens is al langer onderwerp van discu...</td>\n",
       "      <td>ondergren lang onderwerp discussie huidig Nede...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>ondergrens langer onderwerp discussie huidige ...</td>\n",
       "      <td>[(ondergren, VERB), (lang, ADJ), (onderwerp, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>2</th>\n",
       "      <td>Telgenkamp vestigt haar hoop voor de korte ter...</td>\n",
       "      <td>telgenkamp vestigen hoop kort termijn twee cru...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>telgenkamp vestigt hoop korte termijn twee cru...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <th>1</th>\n",
       "      <td>Waarom verzekeraars inkomsten uit zwart werk w...</td>\n",
       "      <td>verzekeraar inkomst zwart werk vergoeden [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraars inkomsten zwart werk vergoeden [N...</td>\n",
       "      <td>[(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">120</th>\n",
       "      <th>1</th>\n",
       "      <td>Verzekeraar wil klant helpen met zorgbemiddeli...</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verzekeraar wil wachtende patiënt aan snelle z...</td>\n",
       "      <td>verzekeraar wachten patiënt snel zorg helpen [...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar wachtende patiënt snelle zorg help...</td>\n",
       "      <td>[(verzekeraar, NOUN), (wachten, VERB), (patiën...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zorgbemiddeling is geen wondermiddel, maar kan...</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zeggen Haa...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zegt haarl...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             original_text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             Provincies willen aan de slag met versoepeling...   \n",
       "           2             Het draait allemaal om de drempelwaarde voor e...   \n",
       "           3             Met een hogere drempelwaarde zouden minder ver...   \n",
       "           4             In het hoofdlijnenakkoord hebben de vier coali...   \n",
       "           5             De ondergrens is al langer onderwerp van discu...   \n",
       "...                                                                    ...   \n",
       "118        2             Telgenkamp vestigt haar hoop voor de korte ter...   \n",
       "119        1             Waarom verzekeraars inkomsten uit zwart werk w...   \n",
       "120        1             Verzekeraar wil klant helpen met zorgbemiddeli...   \n",
       "           2             Verzekeraar wil wachtende patiënt aan snelle z...   \n",
       "           3             Zorgbemiddeling is geen wondermiddel, maar kan...   \n",
       "\n",
       "                                                                      text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincie willen slag versoepeling stikstofreg...   \n",
       "           2             draaien allemaal drempelwaran stikstofvergunni...   \n",
       "           3             hoog drempelwaard vergunning [NEWLINE] aangevo...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartij afsprek...   \n",
       "           5             ondergren lang onderwerp discussie huidig Nede...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigen hoop kort termijn twee cru...   \n",
       "119        1             verzekeraar inkomst zwart werk vergoeden [NEWL...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachten patiënt snel zorg helpen [...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zeggen Haa...   \n",
       "\n",
       "                                   group  \\\n",
       "article_id paragraph_id                    \n",
       "1          1             Bouw & Vastgoed   \n",
       "           2             Bouw & Vastgoed   \n",
       "           3             Bouw & Vastgoed   \n",
       "           4             Bouw & Vastgoed   \n",
       "           5             Bouw & Vastgoed   \n",
       "...                                  ...   \n",
       "118        2                        Zorg   \n",
       "119        1                        Zorg   \n",
       "120        1                        Zorg   \n",
       "           2                        Zorg   \n",
       "           3                        Zorg   \n",
       "\n",
       "                                                 text before lemmatization  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincies willen slag versoepeling stikstofre...   \n",
       "           2             draait allemaal drempelwaarde stikstofvergunni...   \n",
       "           3             hogere drempelwaarde vergunningen [NEWLINE] aa...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartijen afges...   \n",
       "           5             ondergrens langer onderwerp discussie huidige ...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigt hoop korte termijn twee cru...   \n",
       "119        1             verzekeraars inkomsten zwart werk vergoeden [N...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachtende patiënt snelle zorg help...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zegt haarl...   \n",
       "\n",
       "                                                                  pos_tags  \n",
       "article_id paragraph_id                                                     \n",
       "1          1             [(provincie, NOUN), (willen, VERB), (slag, NOU...  \n",
       "           2             [(draaien, VERB), (allemaal, ADV), (drempelwar...  \n",
       "           3             [(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...  \n",
       "           4             [(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...  \n",
       "           5             [(ondergren, VERB), (lang, ADJ), (onderwerp, N...  \n",
       "...                                                                    ...  \n",
       "118        2             [(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...  \n",
       "119        1             [(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...  \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...  \n",
       "           2             [(verzekeraar, NOUN), (wachten, VERB), (patiën...  \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...  \n",
       "\n",
       "[374 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e0d3381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text</th>\n",
       "      <th>group</th>\n",
       "      <th>text before lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_tags_before</th>\n",
       "      <th>pos_tags_before_lemmatization</th>\n",
       "      <th>pos_tags_after_lemmatization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>Provincies willen aan de slag met versoepeling...</td>\n",
       "      <td>provincies willen slag versoepeling stikstofre...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>provincies willen slag versoepeling stikstofre...</td>\n",
       "      <td>[(provincie, NOUN), (willen, VERB), (slag, NOU...</td>\n",
       "      <td>[(provincies, NOUN), (willen, VERB), (slag, NO...</td>\n",
       "      <td>[(provincies, NOUN), (willen, VERB), (slag, NO...</td>\n",
       "      <td>[(provincies, NOUN), (willen, VERB), (slag, NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Het draait allemaal om de drempelwaarde voor e...</td>\n",
       "      <td>draait allemaal drempelwaarde stikstofvergunni...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>draait allemaal drempelwaarde stikstofvergunni...</td>\n",
       "      <td>[(draaien, VERB), (allemaal, ADV), (drempelwar...</td>\n",
       "      <td>[(draait, VERB), (allemaal, ADV), (drempelwaar...</td>\n",
       "      <td>[(draait, VERB), (allemaal, ADV), (drempelwaar...</td>\n",
       "      <td>[(draait, VERB), (allemaal, ADV), (drempelwaar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Met een hogere drempelwaarde zouden minder ver...</td>\n",
       "      <td>hogere drempelwaarde vergunningen [NEWLINE] aa...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hogere drempelwaarde vergunningen [NEWLINE] aa...</td>\n",
       "      <td>[(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...</td>\n",
       "      <td>[(hogere, ADJ), (drempelwaarde, VERB), (vergun...</td>\n",
       "      <td>[(hogere, ADJ), (drempelwaarde, VERB), (vergun...</td>\n",
       "      <td>[(hogere, ADJ), (drempelwaarde, VERB), (vergun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In het hoofdlijnenakkoord hebben de vier coali...</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartijen afges...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>hoofdlijnenakkoord vier coalitiepartijen afges...</td>\n",
       "      <td>[(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...</td>\n",
       "      <td>[(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...</td>\n",
       "      <td>[(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...</td>\n",
       "      <td>[(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>De ondergrens is al langer onderwerp van discu...</td>\n",
       "      <td>ondergrens langer onderwerp discussie huidige ...</td>\n",
       "      <td>Bouw &amp; Vastgoed</td>\n",
       "      <td>ondergrens langer onderwerp discussie huidige ...</td>\n",
       "      <td>[(ondergren, VERB), (lang, ADJ), (onderwerp, N...</td>\n",
       "      <td>[(ondergrens, NOUN), (langer, ADJ), (onderwerp...</td>\n",
       "      <td>[(ondergrens, NOUN), (langer, ADJ), (onderwerp...</td>\n",
       "      <td>[(ondergrens, NOUN), (langer, ADJ), (onderwerp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <th>2</th>\n",
       "      <td>Telgenkamp vestigt haar hoop voor de korte ter...</td>\n",
       "      <td>telgenkamp vestigt hoop korte termijn twee cru...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>telgenkamp vestigt hoop korte termijn twee cru...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...</td>\n",
       "      <td>[(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <th>1</th>\n",
       "      <td>Waarom verzekeraars inkomsten uit zwart werk w...</td>\n",
       "      <td>verzekeraars inkomsten zwart werk vergoeden [N...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraars inkomsten zwart werk vergoeden [N...</td>\n",
       "      <td>[(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...</td>\n",
       "      <td>[(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...</td>\n",
       "      <td>[(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...</td>\n",
       "      <td>[(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">120</th>\n",
       "      <th>1</th>\n",
       "      <td>Verzekeraar wil klant helpen met zorgbemiddeli...</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar klant helpen zorgbemiddeling [NEWL...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "      <td>[(verzekeraar, ADJ), (klant, NOUN), (helpen, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verzekeraar wil wachtende patiënt aan snelle z...</td>\n",
       "      <td>verzekeraar wachtende patiënt snelle zorg help...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>verzekeraar wachtende patiënt snelle zorg help...</td>\n",
       "      <td>[(verzekeraar, NOUN), (wachten, VERB), (patiën...</td>\n",
       "      <td>[(verzekeraar, ADJ), (wachtende, VERB), (patië...</td>\n",
       "      <td>[(verzekeraar, ADJ), (wachtende, VERB), (patië...</td>\n",
       "      <td>[(verzekeraar, ADJ), (wachtende, VERB), (patië...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zorgbemiddeling is geen wondermiddel, maar kan...</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zegt haarl...</td>\n",
       "      <td>Zorg</td>\n",
       "      <td>zorgbemiddeling wondermiddel helpen zegt haarl...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "      <td>[(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             original_text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             Provincies willen aan de slag met versoepeling...   \n",
       "           2             Het draait allemaal om de drempelwaarde voor e...   \n",
       "           3             Met een hogere drempelwaarde zouden minder ver...   \n",
       "           4             In het hoofdlijnenakkoord hebben de vier coali...   \n",
       "           5             De ondergrens is al langer onderwerp van discu...   \n",
       "...                                                                    ...   \n",
       "118        2             Telgenkamp vestigt haar hoop voor de korte ter...   \n",
       "119        1             Waarom verzekeraars inkomsten uit zwart werk w...   \n",
       "120        1             Verzekeraar wil klant helpen met zorgbemiddeli...   \n",
       "           2             Verzekeraar wil wachtende patiënt aan snelle z...   \n",
       "           3             Zorgbemiddeling is geen wondermiddel, maar kan...   \n",
       "\n",
       "                                                                      text  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincies willen slag versoepeling stikstofre...   \n",
       "           2             draait allemaal drempelwaarde stikstofvergunni...   \n",
       "           3             hogere drempelwaarde vergunningen [NEWLINE] aa...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartijen afges...   \n",
       "           5             ondergrens langer onderwerp discussie huidige ...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigt hoop korte termijn twee cru...   \n",
       "119        1             verzekeraars inkomsten zwart werk vergoeden [N...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachtende patiënt snelle zorg help...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zegt haarl...   \n",
       "\n",
       "                                   group  \\\n",
       "article_id paragraph_id                    \n",
       "1          1             Bouw & Vastgoed   \n",
       "           2             Bouw & Vastgoed   \n",
       "           3             Bouw & Vastgoed   \n",
       "           4             Bouw & Vastgoed   \n",
       "           5             Bouw & Vastgoed   \n",
       "...                                  ...   \n",
       "118        2                        Zorg   \n",
       "119        1                        Zorg   \n",
       "120        1                        Zorg   \n",
       "           2                        Zorg   \n",
       "           3                        Zorg   \n",
       "\n",
       "                                                 text before lemmatization  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             provincies willen slag versoepeling stikstofre...   \n",
       "           2             draait allemaal drempelwaarde stikstofvergunni...   \n",
       "           3             hogere drempelwaarde vergunningen [NEWLINE] aa...   \n",
       "           4             hoofdlijnenakkoord vier coalitiepartijen afges...   \n",
       "           5             ondergrens langer onderwerp discussie huidige ...   \n",
       "...                                                                    ...   \n",
       "118        2             telgenkamp vestigt hoop korte termijn twee cru...   \n",
       "119        1             verzekeraars inkomsten zwart werk vergoeden [N...   \n",
       "120        1             verzekeraar klant helpen zorgbemiddeling [NEWL...   \n",
       "           2             verzekeraar wachtende patiënt snelle zorg help...   \n",
       "           3             zorgbemiddeling wondermiddel helpen zegt haarl...   \n",
       "\n",
       "                                                                  pos_tags  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             [(provincie, NOUN), (willen, VERB), (slag, NOU...   \n",
       "           2             [(draaien, VERB), (allemaal, ADV), (drempelwar...   \n",
       "           3             [(hoog, ADJ), (drempelwaard, NOUN), (vergunnin...   \n",
       "           4             [(hoofdlijnenakkoord, PROPN), (vier, NUM), (co...   \n",
       "           5             [(ondergren, VERB), (lang, ADJ), (onderwerp, N...   \n",
       "...                                                                    ...   \n",
       "118        2             [(telgenkamp, NOUN), (vestigen, VERB), (hoop, ...   \n",
       "119        1             [(verzekeraar, ADJ), (inkomst, NOUN), (zwart, ...   \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...   \n",
       "           2             [(verzekeraar, NOUN), (wachten, VERB), (patiën...   \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...   \n",
       "\n",
       "                                                           pos_tags_before  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             [(provincies, NOUN), (willen, VERB), (slag, NO...   \n",
       "           2             [(draait, VERB), (allemaal, ADV), (drempelwaar...   \n",
       "           3             [(hogere, ADJ), (drempelwaarde, VERB), (vergun...   \n",
       "           4             [(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...   \n",
       "           5             [(ondergrens, NOUN), (langer, ADJ), (onderwerp...   \n",
       "...                                                                    ...   \n",
       "118        2             [(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...   \n",
       "119        1             [(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...   \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...   \n",
       "           2             [(verzekeraar, ADJ), (wachtende, VERB), (patië...   \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...   \n",
       "\n",
       "                                             pos_tags_before_lemmatization  \\\n",
       "article_id paragraph_id                                                      \n",
       "1          1             [(provincies, NOUN), (willen, VERB), (slag, NO...   \n",
       "           2             [(draait, VERB), (allemaal, ADV), (drempelwaar...   \n",
       "           3             [(hogere, ADJ), (drempelwaarde, VERB), (vergun...   \n",
       "           4             [(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...   \n",
       "           5             [(ondergrens, NOUN), (langer, ADJ), (onderwerp...   \n",
       "...                                                                    ...   \n",
       "118        2             [(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...   \n",
       "119        1             [(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...   \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...   \n",
       "           2             [(verzekeraar, ADJ), (wachtende, VERB), (patië...   \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...   \n",
       "\n",
       "                                              pos_tags_after_lemmatization  \n",
       "article_id paragraph_id                                                     \n",
       "1          1             [(provincies, NOUN), (willen, VERB), (slag, NO...  \n",
       "           2             [(draait, VERB), (allemaal, ADV), (drempelwaar...  \n",
       "           3             [(hogere, ADJ), (drempelwaarde, VERB), (vergun...  \n",
       "           4             [(hoofdlijnenakkoord, INTJ), (vier, NUM), (coa...  \n",
       "           5             [(ondergrens, NOUN), (langer, ADJ), (onderwerp...  \n",
       "...                                                                    ...  \n",
       "118        2             [(telgenkamp, NOUN), (vestigt, VERB), (hoop, N...  \n",
       "119        1             [(verzekeraars, NOUN), (inkomsten, NOUN), (zwa...  \n",
       "120        1             [(verzekeraar, ADJ), (klant, NOUN), (helpen, V...  \n",
       "           2             [(verzekeraar, ADJ), (wachtende, VERB), (patië...  \n",
       "           3             [(zorgbemiddeling, NOUN), (wondermiddel, NOUN)...  \n",
       "\n",
       "[374 rows x 8 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360db14",
   "metadata": {},
   "source": [
    "# POS-tags one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "444f636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_to_features(pos_tags):\n",
    "    \"\"\"Convert list of (word, POS) tuples into a dictionary of POS tag counts.\"\"\"\n",
    "    pos_counts = Counter(tag for _, tag in pos_tags)\n",
    "    return dict(pos_counts)\n",
    "\n",
    "# df_clean_experimental[\"pos_features\"] = df_clean_experimental[\"pos_tags_after_lemmatization\"].apply(pos_to_features)\n",
    "\n",
    "# # Convert to feature matrix\n",
    "# pos_vectorizer = DictVectorizer(sparse=True)\n",
    "# X_pos = pos_vectorizer.fit_transform(df_clean_experimental[\"pos_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85d58093",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean_experimental[['text', 'pos_tags_after_lemmatization']]  # Feature: text column\n",
    "# y = df_clean_experimental['group']  # Label: group column\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_clean_experimental[\"group\"])\n",
    "\n",
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split the data into text and POS-tags\n",
    "X_train_text, X_train_pos = X_train[['text']], X_train[['pos_tags_after_lemmatization']]\n",
    "X_val_text, X_val_pos = X_val[['text']], X_val[['pos_tags_after_lemmatization']]\n",
    "X_test_text, X_test_pos = X_test[['text']], X_test[['pos_tags_after_lemmatization']]\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_text = vectorizer.fit_transform(X_train_text)\n",
    "X_val_text = vectorizer.transform(X_val_text)\n",
    "X_test_text = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Convert pos tags to feature matrix\n",
    "pos_vectorizer = DictVectorizer(sparse=True)\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "X_train_pos[\"pos_features\"] = X_train_pos[\"pos_tags_after_lemmatization\"].apply(pos_to_features)\n",
    "X_train_pos = pos_vectorizer.fit_transform(X_train_pos[\"pos_features\"])\n",
    "X_train_pos = scaler.fit_transform(X_train_pos)\n",
    "\n",
    "X_val_pos[\"pos_features\"] = X_val_pos[\"pos_tags_after_lemmatization\"].apply(pos_to_features)\n",
    "X_val_pos = pos_vectorizer.fit_transform(X_val_pos[\"pos_features\"])\n",
    "X_val_pos = scaler.fit_transform(X_val_pos)\n",
    "\n",
    "X_test_pos[\"pos_features\"] = X_test_pos[\"pos_tags_after_lemmatization\"].apply(pos_to_features)\n",
    "X_test_pos = pos_vectorizer.fit_transform(X_test_pos[\"pos_features\"])\n",
    "X_test_pos = scaler.fit_transform(X_test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "449e286d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatching dimensions along axis 0: {1, 261}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m X_train\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\_construct.py:535\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhstack\u001b[39m(blocks, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03m    Stack sparse matrices horizontally (column wise)\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    533\u001b[0m \n\u001b[0;32m    534\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\_construct.py:627\u001b[0m, in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(b, csr_matrix)\n\u001b[0;32m    624\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39mflat)):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;66;03m# stack along columns (axis 1):\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m [[_stack_along_minor_axis(blocks[b, :], \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    628\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M)]   \u001b[38;5;66;03m# must have shape: (M, 1)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(blocks, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# stack along rows (axis 0):\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\_construct.py:627\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(b, csr_matrix)\n\u001b[0;32m    624\u001b[0m                                     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39mflat)):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;66;03m# stack along columns (axis 1):\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m [[\u001b[43m_stack_along_minor_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    628\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M)]   \u001b[38;5;66;03m# must have shape: (M, 1)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(blocks, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# stack along rows (axis 0):\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\_construct.py:464\u001b[0m, in \u001b[0;36m_stack_along_minor_axis\u001b[1;34m(blocks, axis)\u001b[0m\n\u001b[0;32m    462\u001b[0m other_axis_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(b\u001b[38;5;241m.\u001b[39mshape[other_axis] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(other_axis_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMismatching dimensions along axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother_axis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    465\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother_axis_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    466\u001b[0m constant_dim, \u001b[38;5;241m=\u001b[39m other_axis_dims\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# Do the stacking\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatching dimensions along axis 0: {1, 261}"
     ]
    }
   ],
   "source": [
    "# Combine features\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_pos])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd4480a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3bfee",
   "metadata": {},
   "source": [
    "# Vector Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19aeb1",
   "metadata": {},
   "source": [
    "For now, I only test TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5696b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21265da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f31c73",
   "metadata": {},
   "source": [
    "**Now I test BERTje Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_bertje = df_clean.copy()\n",
    "\n",
    "# Load BERTje tokenizer and model\n",
    "MODEL_NAME = \"GroNLP/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels (convert string labels to integers)\n",
    "label_mapping = {label: idx for idx, label in enumerate(df_clean_bertje[\"group\"].unique())}\n",
    "df_clean_bertje[\"label\"] = df_clean_bertje[\"group\"].map(label_mapping)\n",
    "\n",
    "# Split dataset\n",
    "X_train_bertje, X_temp_bertje, y_train_bertje, y_temp_bertje = train_test_split(\n",
    "    df_clean_bertje[\"text\"].tolist(), df_clean_bertje[\"label\"].tolist(), test_size=0.3, random_state=42)\n",
    "X_val_bertje, X_test_bertje, y_val_bertje, y_test_bertje = train_test_split(X_temp_bertje, y_temp_bertje, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e05307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state[:, 0, :].squeeze().numpy()  # CLS token representation\n",
    "\n",
    "# Convert text data into embeddings\n",
    "train_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_train_bertje]).numpy()\n",
    "val_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_val_bertje]).numpy()\n",
    "test_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_test_bertje]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, remove me\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state[:, 0, :].squeeze().numpy()  # CLS token representation\n",
    "\n",
    "# Convert text data into embeddings\n",
    "train_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_train_bertje]).numpy()\n",
    "val_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_val_bertje]).numpy()\n",
    "test_embeddings = torch.stack([torch.tensor(get_bert_embedding(text)) for text in X_test_bertje]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c10e",
   "metadata": {},
   "source": [
    "**Now I test Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7da54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f3b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62085a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23541cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6733448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfb2a12",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33550e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    val_score = classifier.score(X_val_tfidf, y_val)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad1401-d7c4-463a-bd21-198f67aeb3e4",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624cac5-4f4b-418b-936c-d0f610d3de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64904bff-0e5c-472f-98ed-cae003a0af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a99c80",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5be378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the SVM hyperparameters using the validation set\n",
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(X_train_tfidf, y_train)\n",
    "        val_score = classifier.score(X_val_tfidf, y_val)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95893b16-aeb5-4e10-97df-ee9bdb0886e5",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15571664-289b-4fe5-ab4d-51b322a1f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c05a32-11b7-4e41-9aab-22e89e376dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518a342-ca92-46e5-9c1b-2f0e4073b063",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1f5dc-cf03-4992-bbe6-5ba435f37616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Naive Bayes hyperparameters using the validation set\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Test different hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "\n",
    "for fit_prior_value in fit_prior_values:\n",
    "    for alpha in alpha_values:\n",
    "        classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "        classifier.fit(X_train_tfidf, y_train)\n",
    "        val_score = classifier.score(X_val_tfidf, y_val)\n",
    "        print(f\"Alpha: {alpha}, fit_prior: {fit_prior_value}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_alpha = alpha\n",
    "            best_fit_prior = fit_prior_value\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best alpha and fit_prior\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15bca6-f63d-418d-b5f2-165bbb05f774",
   "metadata": {},
   "source": [
    "Now we show the accuracy per class and visualize them as a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94617943-b6cb-4bb2-a169-3f91e644937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f2692-c486-40a8-8c82-306bca7b9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383bce3",
   "metadata": {},
   "source": [
    "# Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6759352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Hyperparameter tuning with LOO-CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for fit_prior_value in fit_prior_values:\n",
    "    for alpha in alpha_values:\n",
    "        scores = []\n",
    "        \n",
    "        for train_index, val_index in loo.split(X_train_tfidf):\n",
    "            X_train_cv, X_val = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "            y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "            \n",
    "            classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "            classifier.fit(X_train_cv, y_train_cv)\n",
    "            \n",
    "            y_pred = classifier.predict(X_val)\n",
    "            scores.append(accuracy_score(y_val, y_pred))\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"Alpha: {alpha}, fit_prior: {fit_prior_value}, LOO-CV Score: {mean_score}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_alpha = alpha\n",
    "            best_fit_prior = fit_prior_value\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best LOO-CV Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This leads to worse performance. Likely since Leave-one-out cross validation tends to create High-Variance Models.\n",
    "# Instead, I will use stratified K-fold cross validation. K will be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68115bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Convert text to TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning with Stratified K-Fold CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_k = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "k_values = [2, 3, 5, 10, 20]  # Different values for K in StratifiedKFold\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for k in k_values:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fit_prior_value in fit_prior_values:\n",
    "        for alpha in alpha_values:\n",
    "            scores = []\n",
    "            \n",
    "            for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                X_train_cv, X_val = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                y_train_cv, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                classifier.fit(X_train_cv, y_train_cv)\n",
    "                \n",
    "                y_pred = classifier.predict(X_val)\n",
    "                scores.append(accuracy_score(y_val, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_alpha = alpha\n",
    "                best_fit_prior = fit_prior_value\n",
    "                best_k = k\n",
    "\n",
    "print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5b144-6262-42e8-b9fd-1fadb2d7180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob\n",
    "\n",
    "for row in y_pred_prob:\n",
    "    formatted_row = [\"{:.4f}\".format(val) for val in row]\n",
    "    print(formatted_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86951f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (assuming they are in the same order as in y_train or y_test)\n",
    "class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "\n",
    "# Calculate per-class accuracy: TP / (TP + FN)\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class along with its name\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Class names (from y_test)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, need to ensure TF-IDF vectorization happens within each fold to prevent leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0af70a-6708-452c-891e-5a49be77aa5c",
   "metadata": {},
   "source": [
    "# TF-IDF vectorization within folds to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865cdd5-4051-4dfc-852c-6f2e38b953b3",
   "metadata": {},
   "source": [
    "Currently, vectorizations occurs over the full training set. <br>\n",
    "However, we train K models, 1 for each fold. <br>\n",
    "This means that for each fold, vectorizations should occur for the training data for that specific fold. <br>\n",
    "This avoids data leakage from our validation set to our training set. <br>\n",
    "Note that this is not strictly needed (since not separately within each fold is usually acceptable), but it should slightly improve performance at the cost of additional runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df_clean is already defined with 'text' and 'group' columns\n",
    "X = df_clean['text']  # Feature: text column\n",
    "y = df_clean['group']  # Label: group column\n",
    "\n",
    "# Split the data into training (85%) and test (15%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning with Stratified K-Fold CV\n",
    "best_alpha = None\n",
    "best_fit_prior = None\n",
    "best_k = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Define hyperparameter values\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "fit_prior_values = [True, False]\n",
    "k_values = [2, 3, 5, 10, 20]  # Different values for K in StratifiedKFold\n",
    "\n",
    "# Try different hyperparameter combinations\n",
    "for k in k_values:\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fit_prior_value in fit_prior_values:\n",
    "        for alpha in alpha_values:\n",
    "            scores = []\n",
    "            \n",
    "            for train_index, val_index in skf.split(X_train, y_train):\n",
    "                # Split the raw text data for the current fold\n",
    "                X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                # Vectorize within the fold: fit on training fold, transform validation fold\n",
    "                vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "                \n",
    "                # Initialize and train the classifier\n",
    "                classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                \n",
    "                # Validate the model\n",
    "                y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_alpha = alpha\n",
    "                best_fit_prior = fit_prior_value\n",
    "                best_k = k\n",
    "\n",
    "print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "\n",
    "# Final model training on the entire training set using the best hyperparameters\n",
    "# Here, we fit the vectorizer on the full training set\n",
    "final_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
    "final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "final_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Transform the test set using the vectorizer fitted on the entire training set\n",
    "X_test_tfidf = final_vectorizer.transform(X_test)\n",
    "y_pred = final_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13dc6b-4a39-4f94-bb7f-fd2bf7034bd6",
   "metadata": {},
   "source": [
    "# Turning model tuning into function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed27e7d-32c3-4907-a710-b238f98107eb",
   "metadata": {},
   "source": [
    "Since the process of tuning a classifier tends to not change much, we create a function for every type of classifier so that we can tune them without needing to re-write the code every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf67cc3-6266-4bbe-9b21-8a578bb374ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_random_forest(df: pd.DataFrame,\n",
    "                       testing_ratio: float = 0.15, \n",
    "                       vectorization_within_folds: bool = False,\n",
    "                       k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the Random Forest using stratified K-fold cross validation\n",
    "    best_depth = None\n",
    "    best_score = 0\n",
    "    depth_values = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "    # depth_values = [5, 10]  # For faster tests\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for depth in depth_values:\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_tfidf, y_train)\n",
    "                y_pred = classifier.predict(X_val_tfidf)\n",
    "                mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                print(f\"K: {k}, Depth: {depth}, Validation Accuracy: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_depth = depth\n",
    "                    best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for depth in depth_values:\n",
    "            scores = []\n",
    "\n",
    "            if vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train, y_train):\n",
    "                    # Split the raw text data for the current fold\n",
    "                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "\n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "\n",
    "            if not vectorization_within_folds:\n",
    "                for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                    X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                \n",
    "                classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "                classifier.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = classifier.predict(X_val_fold)\n",
    "                    \n",
    "            scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "            \n",
    "            mean_score = np.mean(scores)\n",
    "            print(f\"K: {k}, Depth: {depth}, StratifiedKFold Score: {mean_score}\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_depth = depth\n",
    "                best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best depth: {best_depth}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268e976-6eb6-413e-813f-a0b3818839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_SVM(df: pd.DataFrame,\n",
    "             testing_ratio: float = 0.15, \n",
    "             vectorization_within_folds: bool = False,\n",
    "             k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    best_score = 0\n",
    "    \n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]    \n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for C in C_values:\n",
    "                for kernel in kernels:\n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, C: {C}, Kernel: {kernel}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_C = C\n",
    "                        best_kernel = kernel\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for C in C_values:\n",
    "            for kernel in kernels:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, C: {C}, Kernel: {kernel}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_C = C\n",
    "                    best_kernel = kernel\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best C: {best_C}, Best kernel: {best_kernel}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d50d6c-bd14-4c2e-a16e-1680beb21c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_naive_bayes(df: pd.DataFrame,\n",
    "                     testing_ratio: float = 0.15, \n",
    "                     vectorization_within_folds: bool = False,\n",
    "                     k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    # Tune the hyperparameters of the SVM classifier using stratified K-fold cross validation\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    best_score = 0\n",
    "    \n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]  \n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df['text']  # Feature: text column\n",
    "    y = df['group']  # Label: group column\n",
    "    \n",
    "    # Split the data into training (85%) and test (15%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "    if not vectorization_within_folds:\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Try different hyperparameter combinations\n",
    "    for k in k_values:\n",
    "        \n",
    "        if k == 1: # If k=1, we use standard hold-out cross-validation\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=testing_ratio*2, random_state=42) # testing_ratio is multiplied by 2 since it is split into validation and test sets after\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_val_tfidf = vectorizer.transform(X_val)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                for fit_prior_value in fit_prior_values:\n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_tfidf, y_train)\n",
    "                    y_pred = classifier.predict(X_val_tfidf)\n",
    "                    mean_score = accuracy_score(y_val, y_pred)  # Validation accuracy directly\n",
    "                    print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, Validation Accuracy: {mean_score}\")\n",
    "                    \n",
    "                    if mean_score > best_score:\n",
    "                        best_score = mean_score\n",
    "                        best_alpha = alpha\n",
    "                        best_fit_prior = fit_prior_value\n",
    "                        best_k = k\n",
    "                        \n",
    "            # Reset X_train, X_test, X_test_tfidf, y_train and y_test after they were changed for the hold-out cross-validation\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "            X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "            X_test_tfidf = vectorizer.transform(X_test)\n",
    "            continue # Skip the StratifiedKFold part for K=1\n",
    "            \n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            for fit_prior_value in fit_prior_values:\n",
    "                scores = []\n",
    "    \n",
    "                if vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train, y_train):\n",
    "                        # Split the raw text data for the current fold\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "                    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "                    X_train_fold_tfidf = vectorizer.fit_transform(X_train_fold)\n",
    "                    X_val_fold_tfidf = vectorizer.transform(X_val_fold)\n",
    "    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold_tfidf, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold_tfidf)\n",
    "    \n",
    "                if not vectorization_within_folds:\n",
    "                    for train_index, val_index in skf.split(X_train_tfidf, y_train):\n",
    "                        X_train_fold, X_val_fold = X_train_tfidf[train_index], X_train_tfidf[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                    \n",
    "                    classifier = MultinomialNB(alpha=alpha, fit_prior=fit_prior_value)\n",
    "                    classifier.fit(X_train_fold, y_train_fold)\n",
    "                    y_pred = classifier.predict(X_val_fold)\n",
    "                        \n",
    "                scores.append(accuracy_score(y_val_fold, y_pred))\n",
    "                \n",
    "                mean_score = np.mean(scores)\n",
    "                print(f\"K: {k}, Alpha: {alpha}, fit_prior: {fit_prior_value}, StratifiedKFold Score: {mean_score}\")\n",
    "                \n",
    "                if mean_score > best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_alpha = alpha\n",
    "                    best_fit_prior = fit_prior_value\n",
    "                    best_k = k\n",
    "    \n",
    "    print(f\"\\nBest K: {best_k}, Best alpha: {best_alpha}, Best fit_prior: {best_fit_prior}, Best StratifiedKFold Score: {best_score}\")\n",
    "    \n",
    "    # Train the final model using the best hyperparameters\n",
    "    if vectorization_within_folds: # X_train_tfidf has yet to be calculated in this case\n",
    "        vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "        \n",
    "    final_classifier = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n",
    "    final_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = final_classifier.predict(X_test_tfidf)\n",
    "    # y_pred_prob = final_classifier.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nTest Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return final_classifier, best_k, best_depth, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86651e-dcd5-4cfa-a38d-9fd1f520ca81",
   "metadata": {},
   "source": [
    "# Overarching Model Selection Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0ab1c-1675-4995-8f8a-587f90a46875",
   "metadata": {},
   "source": [
    "Expected parameter values: <br>\n",
    "> - _classifier_: Selects the type of classifier from amongst the following: [\"SVM\", \"NB\", \"RF\"].\n",
    "> - _testing_ratio_: The ratio of the data that is reserved for testing. Any floating point in the inclusive interval [0, 1].\n",
    "> > Note that if $k=1$, the size of the validation set is assumed to be equal to the size of the testing set, specified by _testing_ratio_.\n",
    "> - _vectorization_within_folds_: Would you like to vectorize each individual fold rather than vectorizing the entire training set once? [True, False].\n",
    "> - _show_class_accuracy_: Would you like the accuracy per class to be displayed? [True, False].\n",
    "> - _show_confusion_matrix_: Would you like the resulting confusion matrix to be displayed? [True, False].\n",
    "> - _k_values_: All values of k which are tested for stratified k-fold cross validation. Any list containing only positive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcb640-3953-4609-aebd-4d46c44ce5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df: pd.DataFrame,\n",
    "                model_type: str = \"SVM\", \n",
    "                testing_ratio: float = 0.15, \n",
    "                vectorization_within_folds: bool = False, \n",
    "                show_class_accuracy: bool = True, \n",
    "                show_confusion_matrix: bool = True,\n",
    "                k_values: list = [2, 3, 5, 10, 20]):\n",
    "    \"\"\"...\"\"\"\n",
    "\n",
    "    print(f\"Tuning {model_type} classifier with a train/test split of {1-testing_ratio}/{testing_ratio} \\n\")\n",
    "    \n",
    "    #Raise appropriate error message in case of a faulty parameter value\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(f\"Invalid input data. Please ensure df is a Pandas DataFrame\")\n",
    "    if model_type not in [\"SVM\", \"NB\", \"RF\"]:\n",
    "        raise ValueError(f\"Invalid model_type. Choose from {'SVM', 'NB', 'RF'}\")\n",
    "    if testing_ratio < 0 or testing_ratio > 1:\n",
    "        raise ValueError(f\"Invalid testing ratio. Choose a value in the inclusive interval [0,1]\")\n",
    "    if type(vectorization_within_folds) != bool:\n",
    "        raise ValueError(f\"Invalid vectorization_within_folds value. Please ensure vectorization_within_folds is boolean\")\n",
    "    if type(show_class_accuracy) != bool:\n",
    "        raise ValueError(f\"Invalid show_class_accuracy value. Please ensure show_class_accuracy is boolean\")\n",
    "    if type(show_confusion_matrix) != bool:\n",
    "        raise ValueError(f\"Invalid show_confusion_matrix value. Please ensure show_confusion_matrix is boolean\")\n",
    "    if not all(isinstance(x, int) and x > 0 for x in k_values):\n",
    "        raise ValueError(f\"Invalid k_values. Please ensure all entries in k_values are positive integers\")\n",
    "    if 1 in k_values and vectorization_within_folds:\n",
    "        raise ValueError(f\"If k_values contains 1, vectorization_within_folds must be False since k=1 implies standard hold-out cross-validation, for which vectorization_within_folds must be False\")\n",
    "\n",
    "    model_mapping = {\"SVM\": SVC, \"NB\": MultinomialNB , \"RF\": RandomForestClassifier}\n",
    "    ModelClass = model_mapping[model_type]\n",
    "    \n",
    "    #Best classifier performance and number of stratified folds so far\n",
    "    best_score = 0\n",
    "    best_k = None\n",
    "\n",
    "    #RF hyperparameters:\n",
    "    best_depth = None\n",
    "    depth_values = [5, 10, 15, 20, 25, None]\n",
    "\n",
    "    #SVM hyperparameters:\n",
    "    best_kernel = None\n",
    "    best_C = None\n",
    "    kernel_values = ['linear', 'rbf', 'poly']\n",
    "    C_values = [0.1, 1, 10]\n",
    "\n",
    "    #NB hyperparameters:\n",
    "    best_alpha = None\n",
    "    best_fit_prior = None\n",
    "    alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    fit_prior_values = [True, False]\n",
    "    \n",
    "    if model_type == \"SVM\":\n",
    "        results = tune_SVM(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"NB\":\n",
    "        results = tune_naive_bayes(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "\n",
    "    if model_type == \"RF\":\n",
    "        results = tune_random_forest(df=df, testing_ratio=testing_ratio, vectorization_within_folds=vectorization_within_folds, k_values=k_values)\n",
    "        classifier, y_pred = results[0], results[3]\n",
    "    \n",
    "    if show_class_accuracy:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Class names (assuming they are in the same order as in y_train or y_test)\n",
    "        class_names = np.unique(y_test)  # This will give you the unique class labels\n",
    "        \n",
    "        # Calculate per-class accuracy: TP / (TP + FN)\n",
    "        class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        # Print the accuracy for each class along with its name\n",
    "        for i, acc in enumerate(class_accuracies):\n",
    "            print(f\"Class '{class_names[i]}' Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        class_names = np.unique(y_test)\n",
    "        \n",
    "        # Plotting the confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
    "        \n",
    "        # Label the axes\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b44d4-078b-44ac-bc5c-10d0d16fcfe5",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95f2ca-af2d-44b3-bd2a-fad6ae19802c",
   "metadata": {},
   "source": [
    "The code below serves only to test the _train_model_ function and to detect and remove bugs. <br>\n",
    "The specific parameter values hold no significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501c74e-df1a-4709-b7b9-bc3980e153a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"RF\", \n",
    "            testing_ratio = 0.15, \n",
    "            vectorization_within_folds = False, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533dcd9-eb7c-4070-a617-7efc2b599430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"SVM\", \n",
    "            testing_ratio = 0.15, \n",
    "            vectorization_within_folds = False, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True,\n",
    "            k_values = [1,2,5,10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc203a32-b1c0-4056-85b0-0249736163ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(df=df_clean,\n",
    "            model_type = \"NB\", \n",
    "            testing_ratio = 0.2, \n",
    "            vectorization_within_folds = True, \n",
    "            show_class_accuracy = True, \n",
    "            show_confusion_matrix = True,\n",
    "           k_values = [2,12,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09ee41-232b-4c60-b3d4-4eeecec9d84e",
   "metadata": {},
   "source": [
    "# Showing percentages per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b28023-60c5-4177-bdf9-68b3beb1b03f",
   "metadata": {},
   "source": [
    "First for SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5367b-b2c1-4571-8c00-5d963176d955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bbaae6a-846e-455b-92ce-03b30565214d",
   "metadata": {},
   "source": [
    "Now for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9a95f-b1d1-4c33-b06c-e26ea2cb0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdd1f8-26ca-42d5-a691-df03481a7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: i have noticed that test performance tends to be higher for k>1 even though the validation score would suggest that k=1 is best. \n",
    "#I think this is because stratified cross validation generalizes better.\n",
    "#How do I decide on k? I can't run everything over test set, that would turn test set into 2nd validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e394536-81e2-4f6b-91e4-fe62c4b7cc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc7ed40",
   "metadata": {},
   "source": [
    "# Testing BERTje word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48c9e1",
   "metadata": {},
   "source": [
    "**NOTE**: These are just basic tests to see if the word embeddings hold potential. <br>\n",
    "The option to use word embeddings should be added to the train_model function above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca26669",
   "metadata": {},
   "source": [
    "First we test random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be60e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the depth of the Random Forest using the validation set\n",
    "best_depth = None\n",
    "best_score = 0\n",
    "depths = [5, 10, 15, 20, 25, None]  # Different depths to test\n",
    "\n",
    "for depth in depths:\n",
    "    classifier = RandomForestClassifier(max_depth=depth, random_state=42)\n",
    "    classifier.fit(train_embeddings, y_train_bertje)\n",
    "    val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "    print(f\"Depth: {depth}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"\\nBest Depth: {best_depth}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best depth\n",
    "final_classifier = RandomForestClassifier(max_depth=best_depth, random_state=42)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acdd27",
   "metadata": {},
   "source": [
    "Now we Test SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel = None\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Test different kernels and values of C (Regularization parameter)\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "C_values = [0.1, 1, 10]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in C_values:\n",
    "        classifier = SVC(kernel=kernel, C=C, random_state=42)\n",
    "        classifier.fit(train_embeddings, y_train_bertje)\n",
    "        val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "        print(f\"Kernel: {kernel}, C: {C}, Validation Score: {val_score}\")\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_kernel = kernel\n",
    "            best_C = C\n",
    "\n",
    "print(f\"\\nBest Kernel: {best_kernel}, Best C: {best_C}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best kernel and C\n",
    "final_classifier = SVC(kernel=best_kernel, C=best_C, random_state=42)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef5695",
   "metadata": {},
   "source": [
    "Now we test Naive Bayes (We test GaussianNB, since MultinomialNB does not work for continuous features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6240373",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "best_var_smoothing = None\n",
    "best_score = float('-inf')\n",
    "\n",
    "# Test different hyperparameter values\n",
    "var_smoothing_values = [10**-11, 10**-10, 10**-9, 10**-8, 10**-7]\n",
    "\n",
    "for var_smoothing in var_smoothing_values:\n",
    "    classifier = GaussianNB(var_smoothing=var_smoothing)\n",
    "    classifier.fit(train_embeddings, y_train_bertje)\n",
    "    val_score = classifier.score(val_embeddings, y_val_bertje)\n",
    "    print(f\"Var_smoothing: {var_smoothing}, Validation Score: {val_score}\")\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_var_smoothing = var_smoothing\n",
    "\n",
    "print(f\"\\nBest var_smoothing: {best_var_smoothing}, Best Validation Score: {best_score}\")\n",
    "\n",
    "# Train the final model using the best alpha and fit_prior\n",
    "final_classifier = GaussianNB(var_smoothing=best_var_smoothing)\n",
    "final_classifier.fit(train_embeddings, y_train_bertje)\n",
    "\n",
    "# Test the model\n",
    "y_pred = final_classifier.predict(test_embeddings)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_bertje, y_pred))\n",
    "print(\"\\nTest Classification Report:\\n\", classification_report(y_test_bertje, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28f230",
   "metadata": {},
   "source": [
    "Surprisingly, these models all perform worse than their tf-idf counterparts. <br>\n",
    "**TO DO**: Rename train_embeddings, val_embeddings, test_embeddings to X_train_bertje, X_val_bertje, X_test_bertje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2ae40",
   "metadata": {},
   "source": [
    "**Idea**: Once pos_tags are used for classification, test and compare performance for:\n",
    "> - Bag of Words <br>\n",
    "> - TF-IDF <br>\n",
    "> - BERTje <br>\n",
    "> - mBERT (Multilingual BERT) <br>\n",
    "> - RobBERT (Dutch RoBERTa model)\n",
    "> - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
